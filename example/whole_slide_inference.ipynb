{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Whole-slide inference using histomics-stream </h2>\n",
    "\n",
    "This example notebook demonstrates how to use Histomics Stream to perform detection inference on a whole-slide image. Histomics Stream prefetches and queues tiles for inference, and can be used with the predict function of a keras detection model to do serial or parallel inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install histomics_detect\n",
    "!pip install -e /tf/notebooks/histomics_detect\n",
    "\n",
    "# install histomics_stream\n",
    "!pip install -e /tf/notebooks/histomics_stream\n",
    "\n",
    "# add to system path\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/tf/notebooks/histomics_detect/\")\n",
    "sys.path.append(\"/tf/notebooks/histomics_stream/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>Download slide and trained model</h2>\n",
    "\n",
    "This example uses a model trained for nuclei detection in breast cancer (see basic example notebook). To illustrate whole-slide inference we apply this model to a 40X objective magnification image from The Cancer Genome Atlas. We also use a mask of the foreground tissue region to avoid uncessesary inference on background tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset related packages\n",
    "from histomics_detect.models import FasterRCNN\n",
    "import os\n",
    "import pooch\n",
    "import tensorflow as tf\n",
    "\n",
    "# download whole slide image\n",
    "wsi_path = pooch.retrieve(\n",
    "    fname='TCGA-AN-A0G0-01Z-00-DX1.svs',\n",
    "    url='https://northwestern.box.com/shared/static/qelyzb45bigg6sqyumtj8kt2vwxztpzm',\n",
    "    known_hash='d046f952759ff6987374786768fc588740eef1e54e4e295a684f3bd356c8528f',\n",
    "    path=str(pooch.os_cache('pooch')) + os.sep + 'wsi'\n",
    ")\n",
    "\n",
    "# download binary mask image\n",
    "mask_path = pooch.retrieve(\n",
    "    fname='TCGA-AN-A0G0-01Z-00-DX1.mask.png',\n",
    "    url='https://northwestern.box.com/shared/static/2q13q2r83avqjz9glrpt3s3nop6uhi2i',\n",
    "    known_hash='bb657ead9fd3b8284db6ecc1ca8a1efa57a0e9fd73d2ea63ce6053fbd3d65171',\n",
    "    path=str(pooch.os_cache('pooch')) + os.sep + 'wsi'\n",
    ")\n",
    "\n",
    "# download trained model\n",
    "model_path = pooch.retrieve(\n",
    "    fname='tcga_brca_model',\n",
    "    url='https://northwestern.box.com/shared/static/wharwfevgyl6qe60hgwchdml67gucq89',\n",
    "    known_hash='410bc16985d1959472d04c6250b4f6fccf60c5ddd31c910254a0f0a605746132',\n",
    "    path=str(pooch.os_cache('pooch')) + os.sep + 'model',\n",
    "    processor=pooch.Unzip()\n",
    ")\n",
    "model_path = os.path.split(model_path[0])[0]\n",
    "\n",
    "# restore keras model\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={'FasterRCNN': FasterRCNN})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Build Dataset from dictionary of instructions</h2>\n",
    "\n",
    "We use Histomics Stream to create a tf.data.Dataset of tiles for feeding the detection model. This allows prefetching and queueing of tiles directly from the whole-slide image and simplifies multi-GPU inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import histomics_stream as hs\n",
    "\n",
    "# define analysis parameters for a single slide\n",
    "slide = {'filename':  wsi_path,\n",
    "         'slide_name': 'TCGA-AN-A0G0-01Z-00-DX1', #slide name without extention\n",
    "         'slide_group': 'TCGA-AN-A0G0', #used to group multiple slides\n",
    "         'number_pixel_rows_for_chunk': 2048,\n",
    "         'number_pixel_columns_for_chunk': 2048}\n",
    "\n",
    "# add slide to study\n",
    "study = {'version': 'version-1',\n",
    "         'number_pixel_rows_for_tile': 1024,\n",
    "         'number_pixel_columns_for_tile': 1024,\n",
    "         'slides': {'TCGA-AN-A0G0-01Z-00-DX1': slide}}\n",
    "\n",
    "# find the best resolution for each slide given the desired_magnification\n",
    "resolution = hs.configure.FindResolutionForSlide(study, desired_magnification=40, magnification_tolerance=0.02)\n",
    "for slide in study[\"slides\"].values():\n",
    "    resolution(slide)\n",
    "\n",
    "# define grid by adding mask and defining tile overlap \n",
    "grid_and_mask = hs.configure.TilesByGridAndMask(\n",
    "    study,\n",
    "    number_pixel_overlap_rows_for_tile=32,\n",
    "    number_pixel_overlap_columns_for_tile=32,\n",
    "    mask_filename=mask_path\n",
    ")\n",
    "    \n",
    "# apply to all slides - we have 1 slide but show this for \n",
    "for slide in study[\"slides\"].values():\n",
    "    grid_and_mask(slide)\n",
    "        \n",
    "# create dataset\n",
    "ds = hs.tensorflow.CreateTensorFlowDataset()\n",
    "tiles = ds(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Using .predict with a model wrapper</h2>\n",
    "\n",
    ".predict() is the most efficient method for inference, however, it combines results from all tiles into a single array. Since these results are in local tile coordinates, the global position of the tile within the slide needs to be included to locale the nuclei within the slide. To do this we create a simple model wrapper that adds the global tile positions to the tile-based inference results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# define the wrapped model class\n",
    "class WrappedModel(tf.keras.Model):\n",
    "    def __init__(self, model, *args, **kwargs):\n",
    "        super(WrappedModel, self).__init__(*args, **kwargs)\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        boxes = self.model(inputs[0], *args, **kwargs)\n",
    "        x = tf.cast(inputs[1]['tile_left'], tf.float32) * tf.ones((tf.shape(boxes)[0], 1))\n",
    "        y = tf.cast(inputs[1]['tile_top'], tf.float32) * tf.ones((tf.shape(boxes)[0], 1))    \n",
    "        return (tf.concat([boxes, x, y], 1), inputs[1])\n",
    "\n",
    "# wrap the model\n",
    "wrapped_model = WrappedModel(model, name='wrapped_model')\n",
    "\n",
    "# .predict inference\n",
    "start = time.time()\n",
    "inference = wrapped_model.predict(tiles)\n",
    "print('%d nuclei in %d tiles: %.2f seconds' % (inference[0].shape[0],\n",
    "                                               len(inference[1]['tile_top']),\n",
    "                                               time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Parallel inference with .predict</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a strategy that mirrors the model across GPUs\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# reformat tiled dataset to replace 'None' values with dummy zeros\n",
    "padded = tiles.map(lambda x, y, z: (x, 0., 0.))\n",
    "\n",
    "# change wrapper class to handle batch dimension, empty batches, and to capture tile location\n",
    "class WrappedModel(tf.keras.Model):\n",
    "    def __init__(self, model, *args, **kwargs):\n",
    "        super(WrappedModel, self).__init__(*args, **kwargs)\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        boxes = tf.cond(tf.greater(tf.size(inputs[0]), 0),\n",
    "                        lambda: self.model(inputs[0][0,:,:,:], *args, **kwargs),\n",
    "                        lambda: tf.zeros((0,4)))\n",
    "        x = tf.cast(inputs[1]['tile_left'], tf.float32) * tf.ones((tf.shape(boxes)[0], 1))\n",
    "        y = tf.cast(inputs[1]['tile_top'], tf.float32) * tf.ones((tf.shape(boxes)[0], 1))    \n",
    "        return (tf.concat([boxes, x, y], 1), inputs[1])\n",
    "\n",
    "# restore and wrap the model in a distributed strategy context\n",
    "with strategy.scope():\n",
    "    \n",
    "    # restore keras model in distributed strategy\n",
    "    model = tf.keras.models.load_model(model_path, custom_objects={'FasterRCNN': FasterRCNN})\n",
    "\n",
    "    # wrap the model\n",
    "    wrapped = WrappedModel(model, name='wrapped_model')\n",
    "    \n",
    "# batch to number of GPUs in strategy\n",
    "batched = padded.batch(strategy.num_replicas_in_sync)\n",
    "\n",
    "# .predict inference\n",
    "start = time.time()\n",
    "inference = wrapped.predict(batched)\n",
    "print('%d nuclei in %d tiles: %.2f seconds on %d GPUs' % (inference[0].shape[0],\n",
    "                                                          len(inference[1]['tile_top']),\n",
    "                                                          time.time()-start,\n",
    "                                                          strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Comparison to list comprehension with tf.data.Dataset.map()</h3>\n",
    "\n",
    "Note: this can be extremely slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from histomics_detect.visualization import plot_inference\n",
    "\n",
    "dataset_map_options = {\n",
    "    \"num_parallel_calls\": tf.data.experimental.AUTOTUNE,\n",
    "    \"deterministic\": False,\n",
    "}\n",
    "\n",
    "# define a dataset containing the input image, the tile inference results, and tile metadata\n",
    "tiles = tiles.map(lambda x, y, z: (x[0], model(x[0], tau=0.5, nms_iou=0.2, margin=32), x[1]), **dataset_map_options)\n",
    "\n",
    "# pull the tiles from the dataset using list comprehension\n",
    "start = time.time()\n",
    "results = [result for result in tiles]\n",
    "print('list comprehension: %.2f' % (time.time()-start))\n",
    "\n",
    "# visualize the tile with the largest number of detections\n",
    "detections = [result[1].shape[0] for result in results]\n",
    "index = detections.index(max(detections))\n",
    "plot_inference(results[index][0], results[index][1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FasterRCNN_ragged.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
