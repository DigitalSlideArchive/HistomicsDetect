{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import packages and install histomics_detect</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install histomics_detect\n",
    "!pip install -e /tf/notebooks/histomics_detect\n",
    "\n",
    "# install histomics_stream\n",
    "!pip install -e /tf/notebooks/histomics_stream\n",
    "\n",
    "# add to system path\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/tf/notebooks/histomics_detect/\")\n",
    "sys.path.append(\"/tf/notebooks/histomics_stream/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# import dataset related packages\n",
    "from histomics_detect.io import dataset\n",
    "from histomics_detect.augmentation import crop, flip, jitter, shrink\n",
    "from histomics_detect.visualization import plot_inference\n",
    "\n",
    "# import whole-slide image handling pipeline\n",
    "import histomics_stream as hs\n",
    "\n",
    "number_epochs = 50  # Set to a number smaller than 50 for speed during debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>Define dataset parameters and create datasets - DCC example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIvluD3u8-We"
   },
   "outputs": [],
   "source": [
    "# input data path\n",
    "path = \"/tf/notebooks/DCC/data/\"\n",
    "\n",
    "# training parameters\n",
    "train_tile = 224  # input image size\n",
    "min_area_thresh = 0.5  # % of object area that must be in random crop to be included\n",
    "width = tf.constant(train_tile, tf.int32)\n",
    "height = tf.constant(train_tile, tf.int32)\n",
    "min_area = tf.constant(min_area_thresh, tf.float32)\n",
    "\n",
    "# split dataset into training and validation\n",
    "cases = [\n",
    "    \"131458\",\n",
    "    \"91315_leica_at2_40x\",\n",
    "    \"135062\",\n",
    "    \"93094\",\n",
    "    \"131453\",\n",
    "    \"131450\",\n",
    "    \"135060\",\n",
    "    \"131463\",\n",
    "    \"131459\",\n",
    "    \"131440\",\n",
    "    \"131460\",\n",
    "    \"93096\",\n",
    "    \"131449\",\n",
    "    \"131457\",\n",
    "    \"131461\",\n",
    "    \"93098\",\n",
    "    \"131447\",\n",
    "    \"93092\",\n",
    "    \"131443\",\n",
    "    \"93095\",\n",
    "    \"131448\",\n",
    "    \"93099\",\n",
    "    \"91316_leica_at2_40x\",\n",
    "    \"131462\",\n",
    "    \"93091\",\n",
    "    \"135065\",\n",
    "    \"131446\",\n",
    "    \"131441\",\n",
    "    \"101626\",\n",
    "    \"93093\",\n",
    "    \"131454\",\n",
    "    \"93097\",\n",
    "    \"131445\",\n",
    "    \"131444\",\n",
    "    \"131456\",\n",
    "    \"93090\",\n",
    "]\n",
    "id = np.argsort(np.random.rand(len(cases) - 1))[0 : np.ceil(0.9 * len(cases)).astype(np.int32)]\n",
    "training = [cases[i] for i in id]\n",
    "validation = list(set(cases).difference(training))\n",
    "\n",
    "# define parser for filenames\n",
    "def parser(file):\n",
    "    name = os.path.splitext(file)[0]\n",
    "    case = name.split(\".\")[2]\n",
    "    roi = \".\".join([name.split(\".\")[1]] + name.split(\".\")[-3:])\n",
    "    return case, roi\n",
    "\n",
    "\n",
    "# generate training, validation datasets\n",
    "ds_train_roi = dataset(path, parser, parser, train_tile, training)\n",
    "ds_validation_roi = dataset(path, parser, parser, 0, validation)\n",
    "\n",
    "# build training dataset\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*crop(x, y, width, height, min_area_thresh), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*flip(x, y), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, jitter(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, shrink(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# build validation datasets\n",
    "ds_validation_roi = ds_validation_roi.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create and train detection model - DCC example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UR0XaVyYmbAr"
   },
   "outputs": [],
   "source": [
    "# import network generation and training packages\n",
    "from histomics_detect.networks.rpns import rpn\n",
    "from histomics_detect.models.faster_rcnn import FasterRCNN\n",
    "\n",
    "# choices for anchor sizes - all anchors 1:1 aspect ratio\n",
    "anchor_px = tf.constant(\n",
    "    [32, 64, 96], dtype=tf.int32\n",
    ")  # width/height of square anchors in pixels at input mag.\n",
    "\n",
    "# feature network parameters\n",
    "backbone_stride = 1  # strides in feature generation network convolution\n",
    "backbone_blocks = 14  # number of residual blocks to use in backbone\n",
    "backbone_dimension = 256  # number of features generated by rpn convolution\n",
    "\n",
    "# rpn network parameters\n",
    "rpn_kernel = [3]  # kernel size for rpn convolution\n",
    "rpn_act_conv = [\"relu\"]  # activation for rpn convolutional layers\n",
    "\n",
    "# anchor filtering parameters\n",
    "neg_max = 128  # maximum number of negative/positive anchors to keep in each roi\n",
    "pos_max = 128\n",
    "rpn_lmbda = 10.0  # weighting for rpn regression loss\n",
    "roialign_tiles = 3.0  # roialign - number of horizontal/vertical tiles in a proposal\n",
    "roialing_pool = 2.0  # roialign - number of horizontal/vertical samples in each tile\n",
    "\n",
    "# create backbone and rpn networks\n",
    "resnet50 = tf.keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=(train_tile, train_tile, 3),\n",
    "    pooling=None,\n",
    ")\n",
    "rpnetwork, backbone = rpn(\n",
    "    resnet50,\n",
    "    n_anchors=tf.size(anchor_px),\n",
    "    stride=backbone_stride,\n",
    "    blocks=backbone_blocks,\n",
    "    kernels=rpn_kernel,\n",
    "    dimensions=[backbone_dimension],\n",
    "    activations=rpn_act_conv,\n",
    ")\n",
    "\n",
    "# create FasterRCNN keras model\n",
    "model = FasterRCNN(rpnetwork, backbone, [width, height], anchor_px, rpn_lmbda)\n",
    "\n",
    "# compile FasterRCNN model with losses\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=[\n",
    "        tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        tf.keras.losses.Huber(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# fit FasterRCNN model\n",
    "model.fit(\n",
    "    x=ds_train_roi,\n",
    "    batch_size=1,\n",
    "    epochs=number_epochs,\n",
    "    verbose=1,\n",
    "    validation_data=ds_validation_roi,\n",
    "    validation_freq=number_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define dataset parameters and create datasets - DLBCL example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset related packages\n",
    "from histomics_detect.io import dataset, resize\n",
    "from histomics_detect.augmentation import crop, flip, jitter, shrink\n",
    "from histomics_detect.visualization import plot_inference\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# input data path\n",
    "path = \"/tf/notebooks/DLBCL/detection/\"\n",
    "\n",
    "# training parameters\n",
    "train_tile = 224  # input image size\n",
    "min_area_thresh = 0.5  # % of object area that must be in crop to be included\n",
    "width = tf.constant(train_tile, tf.int32)\n",
    "height = tf.constant(train_tile, tf.int32)\n",
    "min_area = tf.constant(min_area_thresh, tf.float32)\n",
    "\n",
    "# define filename parsers\n",
    "def png_parser(png):\n",
    "    file = os.path.splitext(png)[0]\n",
    "    case = file.split(\".\")[0]\n",
    "    roi = \".\".join(file.split(\".\")[1:])\n",
    "    return case, roi\n",
    "\n",
    "\n",
    "def csv_parser(csv):\n",
    "    file = os.path.splitext(csv)[0]\n",
    "    case = file.split(\".\")[0]\n",
    "    roi = \".\".join(file.split(\".\")[1:2] + file.split(\".\")[-3:])\n",
    "    return case, roi\n",
    "\n",
    "\n",
    "training = [\n",
    "    \"DCBT_2_CMYC\",\n",
    "    \"DCBT_3_CMYC\",\n",
    "    \"DCBT_5_CMYC\",\n",
    "    \"DCBT_9_CMYC\",\n",
    "    \"DCBT_10_CMYC\",\n",
    "    \"DCBT_12_CMYC\",\n",
    "    \"DCBT_14_CMYC\",\n",
    "    \"DCBT_18_CMYC\",\n",
    "    \"DCBT_19_CMYC\",\n",
    "    \"DCBT_20_CMYC\",\n",
    "    \"DCBT_21_CMYC\",\n",
    "    \"DCBT_22_CMYC\",\n",
    "]\n",
    "validation = [\n",
    "    \"DCBT_1_CMYC\",\n",
    "    \"DCBT_4_CMYC\",\n",
    "    \"DCBT_6_CMYC\",\n",
    "    \"DCBT_8_CMYC\",\n",
    "    \"DCBT_11_CMYC\",\n",
    "    \"DCBT_13_CMYC\",\n",
    "    \"DCBT_15_CMYC\",\n",
    "    \"DCBT_16_CMYC\",\n",
    "    \"DCBT_17_CMYC\",\n",
    "]\n",
    "\n",
    "\n",
    "# generate training, validation datasets\n",
    "ds_train_roi = dataset(path, png_parser, csv_parser, train_tile, training)\n",
    "ds_validation_roi = dataset(path, png_parser, csv_parser, 0, validation)\n",
    "\n",
    "# build training dataset\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*resize(x, y, 2.0), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*crop(x, y, width, height, min_area_thresh), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*flip(x, y), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, jitter(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, shrink(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# build validation datasets\n",
    "ds_validation_roi = ds_validation_roi.map(lambda x, y, z: (*resize(x, y, 2.0), z))\n",
    "ds_validation_roi = ds_validation_roi.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create and train detection model - DLBCL example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import network generation and training packages\n",
    "from histomics_detect.networks.rpns import rpn\n",
    "from histomics_detect.models.faster_rcnn import FasterRCNN\n",
    "\n",
    "# choices for anchor sizes - all anchors 1:1 aspect ratio\n",
    "anchor_px = tf.constant(\n",
    "    [32, 48, 64], dtype=tf.int32\n",
    ")  # width/height of square anchors in pixels at input mag.\n",
    "\n",
    "# feature network parameters\n",
    "backbone_stride = 1  # strides in feature generation network convolution\n",
    "backbone_blocks = 14  # number of residual blocks to use in backbone\n",
    "backbone_dimension = 256  # number of features generated by rpn convolution\n",
    "\n",
    "# rpn network parameters\n",
    "rpn_kernel = [3]  # kernel size for rpn convolution\n",
    "rpn_act_conv = [\"relu\"]  # activation for rpn convolutional layers\n",
    "\n",
    "# anchor filtering parameters\n",
    "neg_max = 128  # maximum number of negative/positive anchors to keep in each roi\n",
    "pos_max = 128\n",
    "rpn_lmbda = 10.0  # weighting for rpn regression loss\n",
    "roialign_tiles = 3.0  # roialign - number of horizontal/vertical tiles in a proposal\n",
    "roialing_pool = 2.0  # roialign - number of horizontal/vertical samples in each tile\n",
    "\n",
    "# create backbone and rpn networks\n",
    "resnet50 = tf.keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=(train_tile, train_tile, 3),\n",
    "    pooling=None,\n",
    ")\n",
    "rpnetwork, backbone = rpn(\n",
    "    resnet50,\n",
    "    n_anchors=tf.size(anchor_px),\n",
    "    stride=backbone_stride,\n",
    "    blocks=backbone_blocks,\n",
    "    kernels=rpn_kernel,\n",
    "    dimensions=[backbone_dimension],\n",
    "    activations=rpn_act_conv,\n",
    ")\n",
    "\n",
    "# create FasterRCNN keras model\n",
    "model = FasterRCNN(rpnetwork, backbone, [width, height], anchor_px, rpn_lmbda)\n",
    "\n",
    "# compile FasterRCNN model with losses\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=[\n",
    "        tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        tf.keras.losses.Huber(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# fit FasterRCNN model\n",
    "model.fit(\n",
    "    x=ds_train_roi,\n",
    "    batch_size=1,\n",
    "    epochs=number_epochs,\n",
    "    verbose=1,\n",
    "    validation_data=ds_validation_roi,\n",
    "    validation_freq=number_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Inference on a single image - model.call() </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and visualize thresholded, roialign outputs\n",
    "data = ds_validation_roi.shuffle(100).take(1).get_single_element()\n",
    "rgb = tf.cast(data[0], tf.uint8)\n",
    "regressions = model(rgb, tau=0.5, nms_iou=0.3)\n",
    "plot_inference(rgb, regressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Raw inference on a single image - model.raw() </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate raw rpn outputs\n",
    "objectness, boxes, features = model.raw(rgb)\n",
    "\n",
    "# threshold rpn proposals\n",
    "boxes_positive, objectness_positive, positive = model.threshold(boxes, objectness, model.tau)\n",
    "\n",
    "# perform non-max suppression on rpn positive predictions\n",
    "boxes_nms, objectness_nms, selected = model.nms(boxes_positive, objectness_positive, model.nms_iou)\n",
    "\n",
    "# generate roialign predictions for rpn positive predictions\n",
    "align_boxes = model.align(boxes_nms, features, model.field, model.pool, model.tiles)\n",
    "\n",
    "# apply thresholding, nms, and roialign\n",
    "plot_inference(rgb, align_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Batch inference using tf.data.Dataset.map </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping model using data.Dataset.map keeps outputs from different images separate\n",
    "map_output = ds_validation_roi.take(5).map(lambda x, y, z: (model(x), y, z))\n",
    "map_output = [element for element in map_output]\n",
    "\n",
    "# compare to using model.predict which merges the outputs from all images\n",
    "predict_output = model.predict(ds_validation_roi.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Batch evaluation - model.evaluate() </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance evaluation on multiple images from a tf.data.Dataset\n",
    "metrics = model.evaluate(ds_validation_roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Build Dataset from dictionary of instructions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Create a study and insert study-wide information\n",
    "my_study0 = {\"version\": \"version-1\"}\n",
    "my_study0[\"number_pixel_rows_for_tile\"] = 256\n",
    "my_study0[\"number_pixel_columns_for_tile\"] = 256\n",
    "my_slides = my_study0[\"slides\"] = {}\n",
    "\n",
    "# Add a slide to the study, including slide-wide information with it.\n",
    "my_slide0 = my_slides[\"Slide_0\"] = {}\n",
    "my_slide0[\"filename\"] = \"/tf/notebooks/histomics_detect/example/DCBT_10_CMYC.svs\"\n",
    "my_slide0[\"slide_name\"] = \"DCBT_10_CMYC\"\n",
    "my_slide0[\"slide_group\"] = \"DCBT_10\"\n",
    "my_slide0[\"number_pixel_rows_for_chunk\"] = 2048\n",
    "my_slide0[\"number_pixel_columns_for_chunk\"] = 2048\n",
    "\n",
    "# For each slide, find the appropriate resolution given the desired_magnification and magnification_tolerance.  In this\n",
    "# example, we use the same parameters for each slide, but this is not required generally.\n",
    "find_resolution_for_slide = hs.configure.FindResolutionForSlide(my_study0, desired_magnification=20, magnification_tolerance=0.02)\n",
    "for slide in my_study0[\"slides\"].values():\n",
    "    find_resolution_for_slide(slide)\n",
    "print(\"================================================================\")\n",
    "print(f\"my_study0 = {my_study0}\")\n",
    "\n",
    "# We are going to demonstrate several approaches to choosing tiles.  Each approach will start with its own copy of the\n",
    "# my_study0 that we have built so far.\n",
    "\n",
    "if True:\n",
    "    # Demonstrate TilesByGridAndMask without a mask\n",
    "    my_study_tiles_by_grid = copy.deepcopy(my_study0)\n",
    "    tiles_by_grid = hs.configure.TilesByGridAndMask(\n",
    "        my_study_tiles_by_grid,\n",
    "        number_pixel_overlap_rows_for_tile=32,\n",
    "        number_pixel_overlap_columns_for_tile=32,\n",
    "        randomly_select=1000,\n",
    "    )\n",
    "    # We could apply this to a subset of the slides, but we will apply it to all slides in this example.\n",
    "    for slide in my_study_tiles_by_grid[\"slides\"].values():\n",
    "        tiles_by_grid(slide)\n",
    "    # print(\"================================================================\")\n",
    "    print(\"Finished with TilesByGrid\")\n",
    "    # print(f\"my_study_tiles_by_grid = {my_study_tiles_by_grid}\")\n",
    "\n",
    "if False:\n",
    "    # Demonstrate TilesByGridAndMask with a mask\n",
    "    my_study_tiles_by_grid_and_mask = copy.deepcopy(my_study0)\n",
    "    tiles_by_grid_and_mask = hs.configure.TilesByGridAndMask(\n",
    "        my_study_tiles_by_grid_and_mask,\n",
    "        number_pixel_overlap_rows_for_tile=0,\n",
    "        number_pixel_overlap_columns_for_tile=0,\n",
    "        mask_filename=\"/tf/notebooks/histomics_stream/example/TCGA-BH-A0BZ-01Z-00-DX1.45EB3E93-A871-49C6-9EAE-90D98AE01913-mask.png\",\n",
    "        randomly_select=1000,\n",
    "    )\n",
    "    # We could apply this to a subset of the slides, but we will apply it to all slides in this example.\n",
    "    for slide in my_study_tiles_by_grid_and_mask[\"slides\"].values():\n",
    "        tiles_by_grid_and_mask(slide)\n",
    "    # print(\"================================================================\")\n",
    "    print(\"Finished with TilesByGridAndMask\")\n",
    "    # print(f\"my_study_tiles_by_grid_and_mask = {my_study_tiles_by_grid_and_mask}\")\n",
    "\n",
    "if True:\n",
    "    # Demonstrate TilesByList\n",
    "    my_study_tiles_by_list = copy.deepcopy(my_study0)\n",
    "    tiles_by_list = hs.configure.TilesByList(\n",
    "        my_study_tiles_by_list, randomly_select=5, tiles_dictionary=my_study_tiles_by_grid[\"slides\"][\"Slide_0\"][\"tiles\"]\n",
    "    )\n",
    "    # We could apply this to a subset of the slides, but we will apply it to all slides in this example.\n",
    "    for slide in my_study_tiles_by_list[\"slides\"].values():\n",
    "        tiles_by_list(slide)\n",
    "    # print(\"================================================================\")\n",
    "    print(\"Finished with TilesByList\")\n",
    "    # print(f\"my_study_tiles_by_list = {my_study_tiles_by_list}\")\n",
    "\n",
    "if True:\n",
    "    # Demonstrate TilesRandomly\n",
    "    my_study_tiles_randomly = copy.deepcopy(my_study0)\n",
    "    tiles_randomly = hs.configure.TilesRandomly(my_study_tiles_randomly, randomly_select=3)\n",
    "    # We could apply this to a subset of the slides, but we will apply it to all slides in this example.\n",
    "    for slide in my_study_tiles_randomly[\"slides\"].values():\n",
    "        tiles_randomly(slide)\n",
    "    # print(\"================================================================\")\n",
    "    print(\"Finished with TilesRandomly\")\n",
    "    # print(f\"my_study_tiles_randomly = {my_study_tiles_randomly}\")\n",
    "\n",
    "# We choose one of the above examples for further processing.\n",
    "my_study_of_tiles = my_study_tiles_by_grid\n",
    "# my_study_of_tiles = my_study_tiles_randomly\n",
    "\n",
    "create_tensorflow_dataset = hs.tensorflow.CreateTensorFlowDataset()\n",
    "tiles = create_tensorflow_dataset(my_study_of_tiles)\n",
    "print(\"Finished with CreateTensorFlowDataset\")\n",
    "\n",
    "# print(\"================================================================\")\n",
    "# print(tiles)\n",
    "# print(\"================================================================\")\n",
    "# tf.print(tiles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Run with the tiles dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_map_options = {\n",
    "    \"num_parallel_calls\": tf.data.experimental.AUTOTUNE,\n",
    "    \"deterministic\": False,\n",
    "}\n",
    "\n",
    "# Convert pixel data to uint8\n",
    "tiles = tiles.map(lambda x, y: (tf.cast(x, tf.uint8), y), **dataset_map_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on the tiles\n",
    "tiles1 = tiles.map(lambda x, y: (x, model(x, tau=0.5, nms_iou=0.3), y), **dataset_map_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>Find a tile with many detections</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles2 = tiles1\n",
    "tiles2 = tiles2.map(lambda x, p, y: (x, p, tf.shape(p)[0], y), **dataset_map_options)\n",
    "\n",
    "max_number_detections = -1\n",
    "number_tiles = 0\n",
    "for tile in tiles2:\n",
    "    number_tiles = number_tiles + 1\n",
    "    rgb, regressions, number_detections, _ = tile\n",
    "    if number_detections > max_number_detections:\n",
    "        tf.print(f\"New best: Tile #{number_tiles} has {number_detections} detections.\")\n",
    "        max_number_detections = number_detections\n",
    "        max_rgb = rgb\n",
    "        max_regressions = regressions\n",
    "    if max_number_detections ** 2 * number_tiles >= 10000:\n",
    "        break\n",
    "tf.print(f\"Examined {number_tiles} tiles in total.\")\n",
    "plot_inference(max_rgb, max_regressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Save and Load Model Weights</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save checkpoint\n",
    "model.save_weights(\"/tf/notebooks/histomics_detect/example/saved_model/\")\n",
    "\n",
    "# create dummy network for restore\n",
    "restored = FasterRCNN(rpnetwork, backbone, [width, height], anchor_px, rpn_lmbda)\n",
    "restored.load_weights(\"/tf/notebooks/histomics_detect/example/saved_model/\")\n",
    "\n",
    "# check that outputs are same\n",
    "assert tf.math.reduce_all(tf.math.equal(restored(rgb), model(rgb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Wrap the model so that predictions can be done with annotations</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedModel(tf.keras.Model):\n",
    "    def __init__(self, model, *args, **kwargs):\n",
    "        super(WrappedModel, self).__init__(*args, **kwargs)\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, pair, *args, **kwargs):\n",
    "        return (self.model(pair[0], *args, **kwargs), pair[1])\n",
    "\n",
    "wrapped_model = WrappedModel(model, name=\"wrapped_model\")\n",
    "wrapped_tiles = tiles.map(lambda rgb, annot: ((rgb, annot), None, None), **dataset_map_options)\n",
    "print(\"Starting wrapped_model.predict\")\n",
    "wrapped_predict_output = wrapped_model.predict(wrapped_tiles)\n",
    "print(\"Finished wrapped_model.predict\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FasterRCNN_ragged.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
