{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import packages and install histomics_detect</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install histomics_detect\n",
    "!pip install -e /tf/notebooks/histomics_detect\n",
    "\n",
    "# install histomics_stream\n",
    "!pip install -e /tf/notebooks/histomics_stream\n",
    "\n",
    "# add to system path\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/tf/notebooks/histomics_detect/\")\n",
    "sys.path.append(\"/tf/notebooks/histomics_stream/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# import dataset related packages\n",
    "from histomics_detect.io import dataset\n",
    "from histomics_detect.augmentation import crop, flip, jitter, shrink\n",
    "from histomics_detect.visualization import plot_inference\n",
    "\n",
    "# import whole-slide image handling pipeline\n",
    "import histomics_stream as hs\n",
    "\n",
    "number_epochs = 50  # Set to a number smaller than 50 for speed during debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>Define dataset parameters and create datasets - DCC example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIvluD3u8-We"
   },
   "outputs": [],
   "source": [
    "# input data path\n",
    "path = \"/tf/notebooks/DCC/data/\"\n",
    "\n",
    "# training parameters\n",
    "train_tile = 224  # input image size\n",
    "min_area_thresh = 0.5  # % of object area that must be in random crop to be included\n",
    "width = tf.constant(train_tile, tf.int32)\n",
    "height = tf.constant(train_tile, tf.int32)\n",
    "min_area = tf.constant(min_area_thresh, tf.float32)\n",
    "\n",
    "# split dataset into training and validation\n",
    "cases = [\n",
    "    \"131458\",\n",
    "    \"91315_leica_at2_40x\",\n",
    "    \"135062\",\n",
    "    \"93094\",\n",
    "    \"131453\",\n",
    "    \"131450\",\n",
    "    \"135060\",\n",
    "    \"131463\",\n",
    "    \"131459\",\n",
    "    \"131440\",\n",
    "    \"131460\",\n",
    "    \"93096\",\n",
    "    \"131449\",\n",
    "    \"131457\",\n",
    "    \"131461\",\n",
    "    \"93098\",\n",
    "    \"131447\",\n",
    "    \"93092\",\n",
    "    \"131443\",\n",
    "    \"93095\",\n",
    "    \"131448\",\n",
    "    \"93099\",\n",
    "    \"91316_leica_at2_40x\",\n",
    "    \"131462\",\n",
    "    \"93091\",\n",
    "    \"135065\",\n",
    "    \"131446\",\n",
    "    \"131441\",\n",
    "    \"101626\",\n",
    "    \"93093\",\n",
    "    \"131454\",\n",
    "    \"93097\",\n",
    "    \"131445\",\n",
    "    \"131444\",\n",
    "    \"131456\",\n",
    "    \"93090\",\n",
    "]\n",
    "id = np.argsort(np.random.rand(len(cases) - 1))[0 : np.ceil(0.9 * len(cases)).astype(np.int32)]\n",
    "training = [cases[i] for i in id]\n",
    "validation = list(set(cases).difference(training))\n",
    "\n",
    "# define parser for filenames\n",
    "def parser(file):\n",
    "    name = os.path.splitext(file)[0]\n",
    "    case = name.split(\".\")[2]\n",
    "    roi = \".\".join([name.split(\".\")[1]] + name.split(\".\")[-3:])\n",
    "    return case, roi\n",
    "\n",
    "\n",
    "# generate training, validation datasets\n",
    "ds_train_roi = dataset(path, parser, parser, train_tile, training)\n",
    "ds_validation_roi = dataset(path, parser, parser, 0, validation)\n",
    "\n",
    "# build training dataset\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*crop(x, y, width, height, min_area_thresh), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*flip(x, y), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, jitter(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, shrink(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# build validation datasets\n",
    "ds_validation_roi = ds_validation_roi.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create and train detection model - DCC example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UR0XaVyYmbAr"
   },
   "outputs": [],
   "source": [
    "# import network generation and training packages\n",
    "from histomics_detect.networks.rpns import rpn\n",
    "from histomics_detect.models.faster_rcnn import FasterRCNN\n",
    "\n",
    "# choices for anchor sizes - all anchors 1:1 aspect ratio\n",
    "anchor_px = tf.constant(\n",
    "    [32, 64, 96], dtype=tf.int32\n",
    ")  # width/height of square anchors in pixels at input mag.\n",
    "\n",
    "# feature network parameters\n",
    "backbone_stride = 1  # strides in feature generation network convolution\n",
    "backbone_blocks = 14  # number of residual blocks to use in backbone\n",
    "backbone_dimension = 256  # number of features generated by rpn convolution\n",
    "\n",
    "# rpn network parameters\n",
    "rpn_kernel = [3]  # kernel size for rpn convolution\n",
    "rpn_act_conv = [\"relu\"]  # activation for rpn convolutional layers\n",
    "\n",
    "# anchor filtering parameters\n",
    "neg_max = 128  # maximum number of negative/positive anchors to keep in each roi\n",
    "pos_max = 128\n",
    "rpn_lmbda = 10.0  # weighting for rpn regression loss\n",
    "roialign_tiles = 3.0  # roialign - number of horizontal/vertical tiles in a proposal\n",
    "roialing_pool = 2.0  # roialign - number of horizontal/vertical samples in each tile\n",
    "\n",
    "# create backbone and rpn networks\n",
    "resnet50 = tf.keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=(train_tile, train_tile, 3),\n",
    "    pooling=None,\n",
    ")\n",
    "rpnetwork, backbone = rpn(\n",
    "    resnet50,\n",
    "    n_anchors=tf.size(anchor_px),\n",
    "    stride=backbone_stride,\n",
    "    blocks=backbone_blocks,\n",
    "    kernels=rpn_kernel,\n",
    "    dimensions=[backbone_dimension],\n",
    "    activations=rpn_act_conv,\n",
    ")\n",
    "\n",
    "# create FasterRCNN keras model\n",
    "model = FasterRCNN(rpnetwork, backbone, [width, height], anchor_px, rpn_lmbda)\n",
    "\n",
    "# compile FasterRCNN model with losses\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=[\n",
    "        tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        tf.keras.losses.Huber(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# fit FasterRCNN model\n",
    "model.fit(\n",
    "    x=ds_train_roi,\n",
    "    batch_size=1,\n",
    "    epochs=number_epochs,\n",
    "    verbose=1,\n",
    "    validation_data=ds_validation_roi,\n",
    "    validation_freq=number_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define dataset parameters and create datasets - DLBCL example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset related packages\n",
    "from histomics_detect.io import dataset, resize\n",
    "from histomics_detect.augmentation import crop, flip, jitter, shrink\n",
    "from histomics_detect.visualization import plot_inference\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# input data path\n",
    "path = \"/tf/notebooks/DLBCL/detection/\"\n",
    "\n",
    "# training parameters\n",
    "train_tile = 224  # input image size\n",
    "min_area_thresh = 0.5  # % of object area that must be in crop to be included\n",
    "width = tf.constant(train_tile, tf.int32)\n",
    "height = tf.constant(train_tile, tf.int32)\n",
    "min_area = tf.constant(min_area_thresh, tf.float32)\n",
    "\n",
    "# define filename parsers\n",
    "def png_parser(png):\n",
    "    file = os.path.splitext(png)[0]\n",
    "    case = file.split(\".\")[0]\n",
    "    roi = \".\".join(file.split(\".\")[1:])\n",
    "    return case, roi\n",
    "\n",
    "\n",
    "def csv_parser(csv):\n",
    "    file = os.path.splitext(csv)[0]\n",
    "    case = file.split(\".\")[0]\n",
    "    roi = \".\".join(file.split(\".\")[1:2] + file.split(\".\")[-3:])\n",
    "    return case, roi\n",
    "\n",
    "\n",
    "training = [\n",
    "    \"DCBT_2_CMYC\",\n",
    "    \"DCBT_3_CMYC\",\n",
    "    \"DCBT_5_CMYC\",\n",
    "    \"DCBT_9_CMYC\",\n",
    "    \"DCBT_10_CMYC\",\n",
    "    \"DCBT_12_CMYC\",\n",
    "    \"DCBT_14_CMYC\",\n",
    "    \"DCBT_18_CMYC\",\n",
    "    \"DCBT_19_CMYC\",\n",
    "    \"DCBT_20_CMYC\",\n",
    "    \"DCBT_21_CMYC\",\n",
    "    \"DCBT_22_CMYC\",\n",
    "]\n",
    "validation = [\n",
    "    \"DCBT_1_CMYC\",\n",
    "    \"DCBT_4_CMYC\",\n",
    "    \"DCBT_6_CMYC\",\n",
    "    \"DCBT_8_CMYC\",\n",
    "    \"DCBT_11_CMYC\",\n",
    "    \"DCBT_13_CMYC\",\n",
    "    \"DCBT_15_CMYC\",\n",
    "    \"DCBT_16_CMYC\",\n",
    "    \"DCBT_17_CMYC\",\n",
    "]\n",
    "\n",
    "\n",
    "# generate training, validation datasets\n",
    "ds_train_roi = dataset(path, png_parser, csv_parser, train_tile, training)\n",
    "ds_validation_roi = dataset(path, png_parser, csv_parser, 0, validation)\n",
    "\n",
    "# build training dataset\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*resize(x, y, 2.0), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*crop(x, y, width, height, min_area_thresh), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*flip(x, y), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, jitter(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, shrink(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# build validation datasets\n",
    "ds_validation_roi = ds_validation_roi.map(lambda x, y, z: (*resize(x, y, 2.0), z))\n",
    "ds_validation_roi = ds_validation_roi.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create and train detection model - DLBCL example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import network generation and training packages\n",
    "from histomics_detect.networks.rpns import rpn\n",
    "from histomics_detect.models.faster_rcnn import FasterRCNN\n",
    "\n",
    "# choices for anchor sizes - all anchors 1:1 aspect ratio\n",
    "anchor_px = tf.constant(\n",
    "    [32, 48, 64], dtype=tf.int32\n",
    ")  # width/height of square anchors in pixels at input mag.\n",
    "\n",
    "# feature network parameters\n",
    "backbone_stride = 1  # strides in feature generation network convolution\n",
    "backbone_blocks = 14  # number of residual blocks to use in backbone\n",
    "backbone_dimension = 256  # number of features generated by rpn convolution\n",
    "\n",
    "# rpn network parameters\n",
    "rpn_kernel = [3]  # kernel size for rpn convolution\n",
    "rpn_act_conv = [\"relu\"]  # activation for rpn convolutional layers\n",
    "\n",
    "# anchor filtering parameters\n",
    "neg_max = 128  # maximum number of negative/positive anchors to keep in each roi\n",
    "pos_max = 128\n",
    "rpn_lmbda = 10.0  # weighting for rpn regression loss\n",
    "roialign_tiles = 3.0  # roialign - number of horizontal/vertical tiles in a proposal\n",
    "roialing_pool = 2.0  # roialign - number of horizontal/vertical samples in each tile\n",
    "\n",
    "# create backbone and rpn networks\n",
    "resnet50 = tf.keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=(train_tile, train_tile, 3),\n",
    "    pooling=None,\n",
    ")\n",
    "rpnetwork, backbone = rpn(\n",
    "    resnet50,\n",
    "    n_anchors=tf.size(anchor_px),\n",
    "    stride=backbone_stride,\n",
    "    blocks=backbone_blocks,\n",
    "    kernels=rpn_kernel,\n",
    "    dimensions=[backbone_dimension],\n",
    "    activations=rpn_act_conv,\n",
    ")\n",
    "\n",
    "# create FasterRCNN keras model\n",
    "model = FasterRCNN(rpnetwork, backbone, [width, height], anchor_px, rpn_lmbda)\n",
    "\n",
    "# compile FasterRCNN model with losses\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=[\n",
    "        tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        tf.keras.losses.Huber(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# fit FasterRCNN model\n",
    "model.fit(\n",
    "    x=ds_train_roi,\n",
    "    batch_size=1,\n",
    "    epochs=number_epochs,\n",
    "    verbose=1,\n",
    "    validation_data=ds_validation_roi,\n",
    "    validation_freq=number_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Inference on a single image - model.call() </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and visualize thresholded, roialign outputs\n",
    "data = ds_validation_roi.shuffle(100).take(1).get_single_element()\n",
    "rgb = tf.cast(data[0], tf.uint8)\n",
    "regressions = model(rgb, tau=0.5, nms_iou=0.3)\n",
    "plot_inference(rgb, regressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Raw inference on a single image - model.raw() </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate raw rpn outputs\n",
    "objectness, boxes, features = model.raw(rgb)\n",
    "\n",
    "# threshold rpn proposals\n",
    "boxes_positive, objectness_positive, positive = model.threshold(boxes, objectness, model.tau)\n",
    "\n",
    "# perform non-max suppression on rpn positive predictions\n",
    "boxes_nms, objectness_nms, selected = model.nms(boxes_positive, objectness_positive, model.nms_iou)\n",
    "\n",
    "# generate roialign predictions for rpn positive predictions\n",
    "align_boxes = model.align(boxes_nms, features, model.field, model.pool, model.tiles)\n",
    "\n",
    "# apply thresholding, nms, and roialign\n",
    "plot_inference(rgb, align_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Batch inference using tf.data.Dataset.map </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping model using data.Dataset.map keeps outputs from different images separate\n",
    "map_output = ds_validation_roi.take(5).map(lambda x, y, z: (model(x), y, z))\n",
    "map_output = [element for element in map_output]\n",
    "\n",
    "# compare to using model.predict which merges the outputs from all images\n",
    "predict_output = model.predict(ds_validation_roi.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Batch evaluation - model.evaluate() </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance evaluation on multiple images from a tf.data.Dataset\n",
    "metrics = model.evaluate(ds_validation_roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Build Dataset from dictionary of instructions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import itk\n",
    "import math\n",
    "import numpy as np\n",
    "import operator\n",
    "import random\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class FindResolutionForSlide:\n",
    "    \"\"\"A class that computes read parameters for slides.\n",
    "\n",
    "    An instance of class FindResolutionForSlide is a callable that will\n",
    "    add level, factor, number_pixel_rows_for_slide, and number_pixel_columns_for_slide fields to a\n",
    "    slide dictionary.\n",
    "\n",
    "    It inputs are filename (which is read from the slide dictionary),\n",
    "    desired_magnification, and magnification_tolerance.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, study, desired_magnification, magnification_tolerance):\n",
    "        \"\"\"Sanity check the supplied parameters and store them for later use.\"\"\"\n",
    "        # Check values.\n",
    "        if not (\"version\" in study and study[\"version\"] == \"version-1\"):\n",
    "            raise ValueError('study[\"version\"] must exist and be equal to \"version-1\".')\n",
    "        if not (isinstance(desired_magnification, (int, float)) and 0 < desired_magnification):\n",
    "            raise ValueError(f\"desired_magnification ({desired_magnification})\" \" must be a positive number\")\n",
    "        if not (isinstance(magnification_tolerance, (int, float)) and 0 <= magnification_tolerance <= 1):\n",
    "            raise ValueError(f\"magnification_tolerance ({magnification_tolerance})\" \" must be a value in [0, 1]\")\n",
    "\n",
    "        # Save values.\n",
    "        self.desired_magnification = desired_magnification\n",
    "        self.magnification_tolerance = magnification_tolerance\n",
    "\n",
    "    def __call__(self, slide):\n",
    "        \"\"\"Add level, factor, number_pixel_rows_for_slide, and number_pixel_columns_for_slide fields to a\n",
    "        slide dictionary.\n",
    "        \"\"\"\n",
    "        # Check values.\n",
    "        if \"filename\" not in slide:\n",
    "            raise ValueError('slide[\"filename\"] must be already set.')\n",
    "        filename = slide[\"filename\"]\n",
    "\n",
    "        # Do the work.\n",
    "        if re.compile(r\"\\.svs$\").search(filename):\n",
    "            import openslide as os\n",
    "\n",
    "            # read whole-slide image file and create openslide object\n",
    "            os_obj = os.OpenSlide(filename)\n",
    "\n",
    "            # measure objective of level 0\n",
    "            objective = np.float32(os_obj.properties[os.PROPERTY_NAME_OBJECTIVE_POWER])\n",
    "\n",
    "            # calculate magnifications of levels\n",
    "            estimated = np.array(objective / os_obj.level_downsamples)\n",
    "\n",
    "            # Find best level to use and its factor\n",
    "            level, factor = self._get_level_and_factor(\n",
    "                self.desired_magnification, estimated, self.magnification_tolerance\n",
    "            )\n",
    "\n",
    "            # get slide number_pixel_columns_for_slide, number_pixel_rows_for_slide at desired\n",
    "            # magnification. (Note number_pixel_columns_for_slide before number_pixel_rows_for_slide)\n",
    "            number_pixel_columns_for_slide, number_pixel_rows_for_slide = os_obj.level_dimensions[level]\n",
    "\n",
    "        elif re.compile(r\"\\.zarr$\").search(filename):\n",
    "            import zarr\n",
    "\n",
    "            # read whole-slide image and create zarr objects\n",
    "            store = zarr.DirectoryStore(filename)\n",
    "            source_group = zarr.open(store, mode=\"r\")\n",
    "\n",
    "            # measure objective of level 0\n",
    "            objective = np.float32(source_group.attrs[os.PROPERTY_NAME_OBJECTIVE_POWER])\n",
    "\n",
    "            # calculate magnifications of levels\n",
    "            estimated = np.array(objective / source_group.attrs[\"level_downsamples\"])\n",
    "\n",
    "            # Find best level to use and its factor\n",
    "            level, factor = self._get_level_and_factor(\n",
    "                self.desired_magnification, estimated, self.magnification_tolerance\n",
    "            )\n",
    "\n",
    "            # get slide number_pixel_columns_for_slide, number_pixel_rows_for_slide at desired\n",
    "            # magnification. (Note number_pixel_rows_for_slide before number_pixel_columns_for_slide)\n",
    "            number_pixel_rows_for_slide, number_pixel_columns_for_slide = source_group[format(level)].shape[0:2]\n",
    "\n",
    "        else:\n",
    "            from PIL import Image\n",
    "\n",
    "            # We don't know magnifications so assume reasonable values for level and\n",
    "            # factor.\n",
    "            level = 0\n",
    "            factor = 1.0\n",
    "            pil_obj = Image.open(filename)\n",
    "            number_pixel_columns_for_slide, number_pixel_rows_for_slide = pil_obj.size\n",
    "\n",
    "        slide[\"level\"] = level\n",
    "        slide[\"factor\"] = factor\n",
    "        slide[\"number_pixel_rows_for_slide\"] = number_pixel_rows_for_slide\n",
    "        slide[\"number_pixel_columns_for_slide\"] = number_pixel_columns_for_slide\n",
    "\n",
    "    def _get_level_and_factor(self, desired_magnification, estimated, magnification_tolerance):\n",
    "        \"\"\"A private subroutine that computes level and factor.\"\"\"\n",
    "        # calculate difference with magnification levels\n",
    "        delta = desired_magnification - estimated\n",
    "\n",
    "        # match to existing levels\n",
    "        if np.min(np.abs(np.divide(delta, desired_magnification))) < magnification_tolerance:  # match\n",
    "            level = np.squeeze(np.argmin(np.abs(delta)))\n",
    "            factor = 1.0\n",
    "        elif np.any(delta < 0):\n",
    "            value = np.max(delta[delta < 0])\n",
    "            level = np.squeeze(np.argwhere(delta == value)[0])\n",
    "            factor = desired_magnification / estimated[level]\n",
    "        else:  # desired magnification above base level - throw error\n",
    "            raise ValueError(\"Cannot interpolate above scan magnification.\")\n",
    "\n",
    "        return level, factor\n",
    "\n",
    "\n",
    "class TilesByGridAndMask:\n",
    "    \"\"\"Select tiles according to a regular grid.  Optionally, restrict the list by a mask.\n",
    "    Optionally, select a random subset of them.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        study,\n",
    "        randomly_select=-1,  # Defaults to select all\n",
    "        number_pixel_overlap_rows_for_tile=0,  # Defaults to no overlap between adjacent tiles\n",
    "        number_pixel_overlap_columns_for_tile=0,\n",
    "        mask_filename=\"\",  # Defaults to no masking\n",
    "    ):\n",
    "        \"\"\"Sanity check the supplied parameters and store them for later use.\"\"\"\n",
    "        # Check values.\n",
    "        if not (\"version\" in study and study[\"version\"] == \"version-1\"):\n",
    "            raise ValueError('study[\"version\"] must exist and be equal to \"version-1\".')\n",
    "        if not (\n",
    "            \"number_pixel_rows_for_tile\" in study\n",
    "            and isinstance(study[\"number_pixel_rows_for_tile\"], int)\n",
    "            and study[\"number_pixel_rows_for_tile\"] > 0\n",
    "        ):\n",
    "            raise ValueError('study[\"number_pixel_rows_for_tile\"]' \" must exist and be a positive integer\")\n",
    "        if not (\n",
    "            \"number_pixel_columns_for_tile\" in study\n",
    "            and isinstance(study[\"number_pixel_columns_for_tile\"], int)\n",
    "            and study[\"number_pixel_columns_for_tile\"] > 0\n",
    "        ):\n",
    "            raise ValueError('study[\"number_pixel_columns_for_tile\"]' \" must exist and be a positive integer\")\n",
    "        if not (isinstance(randomly_select, int) and -1 <= randomly_select):\n",
    "            raise ValueError(f\"randomly_select ({randomly_select})\" \" must be a non-negative integer or -1.\")\n",
    "        if not (\n",
    "            isinstance(number_pixel_overlap_rows_for_tile, int)\n",
    "            and number_pixel_overlap_rows_for_tile < study[\"number_pixel_rows_for_tile\"]\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"number_pixel_overlap_rows_for_tile ({number_pixel_overlap_rows_for_tile})\"\n",
    "                \" must be less than\"\n",
    "                f' number_pixel_rows_for_tile ({study[\"number_pixel_rows_for_tile\"]}).'\n",
    "            )\n",
    "        if not (\n",
    "            isinstance(number_pixel_overlap_columns_for_tile, int)\n",
    "            and number_pixel_overlap_columns_for_tile < study[\"number_pixel_columns_for_tile\"]\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"number_pixel_overlap_columns_for_tile ({number_pixel_overlap_columns_for_tile})\"\n",
    "                \" must be less than\"\n",
    "                f' number_pixel_columns_for_tile ({study[\"number_pixel_columns_for_tile\"]}).'\n",
    "            )\n",
    "        if mask_filename != \"\":\n",
    "            mask_itk = itk.imread(mask_filename)  # May throw exception\n",
    "            if mask_itk.GetImageDimension() != 2:\n",
    "                raise ValueError(f\"The mask ({mask_filename})\" \" should be a 2-dimensional image.\")\n",
    "\n",
    "        # Save values.  To keep garbage collection efficient don't save all of `study`.\n",
    "        self.number_pixel_rows_for_tile = study[\"number_pixel_rows_for_tile\"]\n",
    "        self.number_pixel_columns_for_tile = study[\"number_pixel_columns_for_tile\"]\n",
    "        self.randomly_select = randomly_select\n",
    "        self.number_pixel_overlap_rows_for_tile = number_pixel_overlap_rows_for_tile\n",
    "        self.number_pixel_overlap_columns_for_tile = number_pixel_overlap_columns_for_tile\n",
    "        self.mask_filename = mask_filename\n",
    "        if mask_filename != \"\":\n",
    "            self.mask_itk = mask_itk\n",
    "\n",
    "    def __call__(self, slide):\n",
    "        \"\"\"Select tiles according to a regular grid.  Optionally, restrict the list by a mask.\n",
    "        Optionally, select a random subset of them.\n",
    "        \"\"\"\n",
    "        # Check values.\n",
    "        if \"number_pixel_rows_for_slide\" not in slide:\n",
    "            raise ValueError('slide[\"number_pixel_rows_for_slide\"] must be already set.')\n",
    "        if \"number_pixel_columns_for_slide\" not in slide:\n",
    "            raise ValueError('slide[\"number_pixel_columns_for_slide\"] must be already set.')\n",
    "\n",
    "        # Do the work.\n",
    "        row_stride = self.number_pixel_rows_for_tile - self.number_pixel_overlap_rows_for_tile\n",
    "        number_tile_rows_for_slide = slide[\"number_tile_rows_for_slide\"] = math.floor(\n",
    "            (slide[\"number_pixel_rows_for_slide\"] - self.number_pixel_overlap_rows_for_tile) / row_stride\n",
    "        )\n",
    "        column_stride = self.number_pixel_columns_for_tile - self.number_pixel_overlap_columns_for_tile\n",
    "        number_tile_columns_for_slide = slide[\"number_tile_columns_for_slide\"] = math.floor(\n",
    "            (slide[\"number_pixel_columns_for_slide\"] - self.number_pixel_overlap_columns_for_tile) / column_stride\n",
    "        )\n",
    "        has_mask = hasattr(self, \"mask_itk\")\n",
    "        if has_mask:\n",
    "            # We will change the resolution of the mask (if\n",
    "            # necessary), which will change the number of pixels, but\n",
    "            # will not change the overall physical size represented by\n",
    "            # the image nor the position of the upper left corner of\n",
    "            # its upper left pixel.\n",
    "            input_size = itk.size(self.mask_itk)\n",
    "            output_size = [number_tile_columns_for_slide, number_tile_rows_for_slide]\n",
    "            if input_size != output_size:\n",
    "                # print(f\"Resampling from input_size = {input_size} to output_size = {output_size}\")\n",
    "                # Check that the input and output aspect ratios are pretty close\n",
    "                if abs(math.log((output_size[0] / input_size[0]) / (output_size[1] / input_size[1]))) > 0.20:\n",
    "                    raise ValueError(\"The mask aspect ratio does not match that for the number of tiles.\")\n",
    "                input_spacing = itk.spacing(self.mask_itk)\n",
    "                input_origin = itk.origin(self.mask_itk)\n",
    "                image_dimension = self.mask_itk.GetImageDimension()\n",
    "                output_spacing = [input_spacing[d] * input_size[d] / output_size[d] for d in range(image_dimension)]\n",
    "                output_origin = [\n",
    "                    input_origin[d] + 0.5 * (output_spacing[d] - input_spacing[d]) for d in range(image_dimension)\n",
    "                ]\n",
    "                interpolator = itk.NearestNeighborInterpolateImageFunction.New(self.mask_itk)\n",
    "                resampled_mask_itk = itk.resample_image_filter(\n",
    "                    self.mask_itk,\n",
    "                    interpolator=interpolator,\n",
    "                    size=output_size,\n",
    "                    output_spacing=output_spacing,\n",
    "                    output_origin=output_origin,\n",
    "                )\n",
    "            else:\n",
    "                resampled_mask_itk = self.mask_itk\n",
    "\n",
    "        tiles = slide[\"tiles\"] = {}\n",
    "        number_of_tiles = 0\n",
    "        for row in range(number_tile_rows_for_slide):\n",
    "            for column in range(number_tile_columns_for_slide):\n",
    "                if not (has_mask and resampled_mask_itk[row, column] == 0):\n",
    "                    tiles[f\"tile_{number_of_tiles}\"] = {\n",
    "                        \"tile_top\": row * row_stride,\n",
    "                        \"tile_left\": column * column_stride,\n",
    "                    }\n",
    "                number_of_tiles += 1  # Increment even if tile is skipped.\n",
    "        # Choose a subset of the tiles randomly\n",
    "        all_tile_names = tiles.keys()\n",
    "        if 0 <= self.randomly_select < len(all_tile_names):\n",
    "            keys_to_remove = random.sample(all_tile_names, len(all_tile_names) - self.randomly_select)\n",
    "            for key in keys_to_remove:\n",
    "                del tiles[key]\n",
    "\n",
    "\n",
    "class TilesByList:\n",
    "    \"\"\"Select the tiles supplied by the user.  Optionally, select a random subset of them.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        study,\n",
    "        randomly_select=-1,  # Defaults to select all\n",
    "        tiles_dictionary={},  # {'AB234': {'tile_top': top0, 'tile_left': left0}, 'CD43': {'tile_top': top1, 'tile_left': left1}, ...}\n",
    "    ):\n",
    "        \"\"\"Sanity check the supplied parameters and store them for later use.\"\"\"\n",
    "        # Check values\n",
    "        if not (\"version\" in study and study[\"version\"] == \"version-1\"):\n",
    "            raise ValueError('study[\"version\"] must exist and be equal to \"version-1\".')\n",
    "        if not (\n",
    "            \"number_pixel_rows_for_tile\" in study\n",
    "            and isinstance(study[\"number_pixel_rows_for_tile\"], int)\n",
    "            and study[\"number_pixel_rows_for_tile\"] > 0\n",
    "        ):\n",
    "            raise ValueError('study[\"number_pixel_rows_for_tile\"]' \" must exist and be a positive integer\")\n",
    "        if not (\n",
    "            \"number_pixel_columns_for_tile\" in study\n",
    "            and isinstance(study[\"number_pixel_columns_for_tile\"], int)\n",
    "            and study[\"number_pixel_columns_for_tile\"] > 0\n",
    "        ):\n",
    "            raise ValueError('study[\"number_pixel_columns_for_tile\"]' \" must exist and be a positive integer\")\n",
    "        if not (isinstance(randomly_select, int) and -1 <= randomly_select):\n",
    "            raise ValueError(f\"randomly_select ({randomly_select})\" \" must be a non-negative integer or -1.\")\n",
    "        if not (\n",
    "            isinstance(tiles_dictionary, dict)\n",
    "            and all([isinstance(tile_corner, dict) for tile_corner in tiles_dictionary.values()])\n",
    "            and all(\n",
    "                [\n",
    "                    key in tile_corner.keys()\n",
    "                    for tile_corner in tiles_dictionary.values()\n",
    "                    for key in (\"tile_top\", \"tile_left\")\n",
    "                ]\n",
    "            )\n",
    "            and all(\n",
    "                [\n",
    "                    isinstance(tile_corner[key], int)\n",
    "                    for tile_corner in tiles_dictionary.values()\n",
    "                    for key in (\"tile_top\", \"tile_left\")\n",
    "                ]\n",
    "            )\n",
    "            and all(\n",
    "                [\n",
    "                    tile_corner[key] >= 0\n",
    "                    for tile_corner in tiles_dictionary.values()\n",
    "                    for key in (\"tile_top\", \"tile_left\")\n",
    "                ]\n",
    "            )\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"tiles_dictionary must be dictionary of tiles.\"\n",
    "                '  Each tile is a dictionary, with keys \"tile_top\" and \"tile_left\"'\n",
    "                \" and with values that are non-negative integers.\"\n",
    "            )\n",
    "\n",
    "        # Save values.  To keep garbage collection efficient don't save all of `study`,\n",
    "        # just the parts that we need.\n",
    "        self.number_pixel_rows_for_tile = study[\"number_pixel_rows_for_tile\"]\n",
    "        self.number_pixel_columns_for_tile = study[\"number_pixel_columns_for_tile\"]\n",
    "        self.randomly_select = randomly_select\n",
    "        self.tiles_dictionary = copy.deepcopy(tiles_dictionary)  # in case user changes it later\n",
    "\n",
    "    def __call__(self, slide):\n",
    "        \"\"\"Select the tiles supplied by the user.  Optionally, select a random subset of them.\"\"\"\n",
    "        tiles = slide[\"tiles\"] = copy.deepcopy(self.tiles_dictionary)  # in case __call__ is called again.\n",
    "        all_tile_names = tiles.keys()\n",
    "        if 0 <= self.randomly_select < len(all_tile_names):\n",
    "            keys_to_remove = random.sample(all_tile_names, len(all_tile_names) - self.randomly_select)\n",
    "            for key in keys_to_remove:\n",
    "                del tiles[key]\n",
    "\n",
    "\n",
    "class TilesRandomly:\n",
    "    \"\"\"Select a random subset of all possible tiles.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        study,\n",
    "        randomly_select=1,  # Defaults to select one\n",
    "    ):\n",
    "        \"\"\"Sanity check the supplied parameters and store them for later use.\"\"\"\n",
    "        # Check values.\n",
    "        if not (\"version\" in study and study[\"version\"] == \"version-1\"):\n",
    "            raise ValueError('study[\"version\"] must exist and be equal to \"version-1\".')\n",
    "        if not (\n",
    "            \"number_pixel_rows_for_tile\" in study\n",
    "            and isinstance(study[\"number_pixel_rows_for_tile\"], int)\n",
    "            and study[\"number_pixel_rows_for_tile\"] > 0\n",
    "        ):\n",
    "            raise ValueError('study[\"number_pixel_rows_for_tile\"]' \" must exist and be a positive integer\")\n",
    "        if not (\n",
    "            \"number_pixel_columns_for_tile\" in study\n",
    "            and isinstance(study[\"number_pixel_columns_for_tile\"], int)\n",
    "            and study[\"number_pixel_columns_for_tile\"] > 0\n",
    "        ):\n",
    "            raise ValueError('study[\"number_pixel_columns_for_tile\"]' \" must exist and be a positive integer\")\n",
    "        if not (isinstance(randomly_select, int) and 0 <= randomly_select):\n",
    "            raise ValueError(f\"randomly_select ({randomly_select})\" \" must be a non-negative integer.\")\n",
    "\n",
    "        # Save values.  To keep garbage collection efficient don't save all of `study`.\n",
    "        self.number_pixel_rows_for_tile = study[\"number_pixel_rows_for_tile\"]\n",
    "        self.number_pixel_columns_for_tile = study[\"number_pixel_columns_for_tile\"]\n",
    "        self.randomly_select = randomly_select\n",
    "\n",
    "    def __call__(self, slide):\n",
    "        \"\"\"Select a random subset of all possible tiles.\"\"\"\n",
    "        if \"number_pixel_rows_for_slide\" not in slide:\n",
    "            raise ValueError('slide[\"number_pixel_rows_for_slide\"] must be already set.')\n",
    "        if \"number_pixel_columns_for_slide\" not in slide:\n",
    "            raise ValueError('slide[\"number_pixel_columns_for_slide\"] must be already set.')\n",
    "\n",
    "        row_too_big = slide[\"number_pixel_rows_for_slide\"] - self.number_pixel_rows_for_tile + 1\n",
    "        column_too_big = slide[\"number_pixel_columns_for_slide\"] - self.number_pixel_columns_for_tile + 1\n",
    "        row_column_list = [\n",
    "            (random.randrange(0, row_too_big), random.randrange(0, column_too_big)) for _ in range(self.randomly_select)\n",
    "        ]\n",
    "        tiles = slide[\"tiles\"] = {}\n",
    "        number_of_tiles = 0\n",
    "        for (row, column) in row_column_list:\n",
    "            tiles[f\"tile_{number_of_tiles}\"] = {\"tile_top\": row, \"tile_left\": column}\n",
    "            number_of_tiles += 1\n",
    "\n",
    "\n",
    "class CreateTensorFlowDataset:\n",
    "    def __init__(self):\n",
    "        self.dataset_map_options = {\n",
    "            \"num_parallel_calls\": tf.data.experimental.AUTOTUNE,\n",
    "            \"deterministic\": False,\n",
    "        }\n",
    "\n",
    "    def __call__(self, study_description):\n",
    "        \"\"\"From scratch, creates a tensorflow dataset with one tensorflow element per tile\"\"\"\n",
    "\n",
    "        if not (\"version\" in study_description and study_description[\"version\"] == \"version-1\"):\n",
    "            raise ValueError('study_description[\"version\"] must exist and be equal to \"version-1\".')\n",
    "        if not (\n",
    "            \"number_pixel_rows_for_tile\" in study_description\n",
    "            and isinstance(study_description[\"number_pixel_rows_for_tile\"], int)\n",
    "            and study_description[\"number_pixel_rows_for_tile\"] > 0\n",
    "        ):\n",
    "            raise ValueError('study_description[\"number_pixel_rows_for_tile\"]' \" must exist and be a positive integer\")\n",
    "        if not (\n",
    "            \"number_pixel_columns_for_tile\" in study_description\n",
    "            and isinstance(study_description[\"number_pixel_columns_for_tile\"], int)\n",
    "            and study_description[\"number_pixel_columns_for_tile\"] > 0\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                'study_description[\"number_pixel_columns_for_tile\"]' \" must exist and be a positive integer\"\n",
    "            )\n",
    "        # Check that other necessary keys are also present!!!\n",
    "\n",
    "        # Partition the set of tiles into chunks.\n",
    "        self._designate_chunks_for_tiles(study_description)\n",
    "        # cProfile.runctx(\"self._designate_chunks_for_tiles(study_description)\", globals=globals(), locals=locals(), sort=\"cumulative\")\n",
    "        # print(\"_designate_chunks_for_tiles done\")\n",
    "\n",
    "        self.number_pixel_rows_for_tile = tf.convert_to_tensor(study_description[\"number_pixel_rows_for_tile\"])\n",
    "        self.number_pixel_columns_for_tile = tf.convert_to_tensor(study_description[\"number_pixel_columns_for_tile\"])\n",
    "\n",
    "        # Start converting our description into tensors.\n",
    "        study_as_tensors = {\n",
    "            study_key: [tf.convert_to_tensor(study_description[study_key])]\n",
    "            for study_key in study_description.keys()\n",
    "            if study_key != \"slides\"\n",
    "        }\n",
    "        # print(\"study_as_tensors done\")\n",
    "\n",
    "        number_of_chunks = 0\n",
    "        for slide_description in study_description[\"slides\"].values():\n",
    "            slide_as_tensors = {\n",
    "                **study_as_tensors,\n",
    "                **{\n",
    "                    slide_key: [tf.convert_to_tensor(slide_description[slide_key])]\n",
    "                    for slide_key in slide_description.keys()\n",
    "                    if slide_key != \"chunks\"\n",
    "                },\n",
    "            }\n",
    "\n",
    "            for chunk_description in slide_description[\"chunks\"].values():\n",
    "                chunk_as_tensors = {\n",
    "                    **slide_as_tensors,\n",
    "                    **{\n",
    "                        chunk_key: [tf.convert_to_tensor(chunk_description[chunk_key])]\n",
    "                        for chunk_key in chunk_description.keys()\n",
    "                        if chunk_key != \"tiles\"\n",
    "                    },\n",
    "                    \"tiles_top\": [\n",
    "                        tf.convert_to_tensor([tile[\"tile_top\"] for tile in chunk_description[\"tiles\"].values()])\n",
    "                    ],\n",
    "                    \"tiles_left\": [\n",
    "                        tf.convert_to_tensor([tile[\"tile_left\"] for tile in chunk_description[\"tiles\"].values()])\n",
    "                    ],\n",
    "                }\n",
    "\n",
    "                # Make a tensorflow Dataset from this chunk.\n",
    "                chunk_dataset = tf.data.Dataset.from_tensor_slices(chunk_as_tensors)\n",
    "                if number_of_chunks == 0:\n",
    "                    study_dataset = chunk_dataset\n",
    "                else:\n",
    "                    study_dataset = study_dataset.concatenate(chunk_dataset)\n",
    "                number_of_chunks += 1\n",
    "\n",
    "        # We have accumulated the chunk datasets into a study_dataset where each element is a chunk.  Read in the chunk\n",
    "        # pixel data and split it into tiles.\n",
    "        study_dataset = study_dataset.map(self._read_and_split_chunk_pixels, **self.dataset_map_options)\n",
    "        # print(\"_read_and_split_chunk_pixels done\")\n",
    "        # Change study_dataset so that each element is a tile.\n",
    "        study_dataset = study_dataset.unbatch()\n",
    "        # print(\"unbatch done\")\n",
    "        # Make the tile pixels easier to find in each study_dataset element.\n",
    "        study_dataset = study_dataset.map(lambda elem: (elem.pop(\"tile_pixels\"), elem), **self.dataset_map_options)\n",
    "        # print(\"pop done\")\n",
    "\n",
    "        return study_dataset\n",
    "\n",
    "    def _designate_chunks_for_tiles(self, study_description):\n",
    "        number_pixel_rows_for_tile = study_description[\"number_pixel_rows_for_tile\"]\n",
    "        number_pixel_columns_for_tile = study_description[\"number_pixel_columns_for_tile\"]\n",
    "\n",
    "        for slide in study_description[\"slides\"].values():\n",
    "            if not (\n",
    "                \"number_pixel_rows_for_chunk\" in slide\n",
    "                and isinstance(slide[\"number_pixel_rows_for_chunk\"], int)\n",
    "                and slide[\"number_pixel_rows_for_chunk\"] > 0\n",
    "            ):\n",
    "                raise ValueError('slide[\"number_pixel_rows_for_chunk\"]' \" must exist and be a positive integer\")\n",
    "            if not (\n",
    "                \"number_pixel_columns_for_chunk\" in slide\n",
    "                and isinstance(slide[\"number_pixel_columns_for_chunk\"], int)\n",
    "                and slide[\"number_pixel_columns_for_chunk\"] > 0\n",
    "            ):\n",
    "                raise ValueError('slide[\"number_pixel_columns_for_chunk\"]' \" must exist and be a positive integer\")\n",
    "            number_pixel_rows_for_chunk = slide[\"number_pixel_rows_for_chunk\"]\n",
    "            number_pixel_columns_for_chunk = slide[\"number_pixel_columns_for_chunk\"]\n",
    "\n",
    "            tiles_as_sorted_list = list(slide[\"tiles\"].items())\n",
    "            del slide[\"tiles\"]\n",
    "            tiles_as_sorted_list.sort(key=lambda x: x[1][\"tile_left\"])  # second priority key\n",
    "            tiles_as_sorted_list.sort(key=lambda x: x[1][\"tile_top\"])  # first priority key\n",
    "            chunks = slide[\"chunks\"] = {}\n",
    "            number_of_chunks = 0\n",
    "            while len(tiles_as_sorted_list) > 0:\n",
    "                tile = tiles_as_sorted_list[0]\n",
    "                chunk = chunks[f\"chunk_{number_of_chunks}\"] = {\n",
    "                    \"chunk_top\": tile[1][\"tile_top\"],\n",
    "                    \"chunk_left\": tile[1][\"tile_left\"],\n",
    "                    \"chunk_bottom\": tile[1][\"tile_top\"] + number_pixel_rows_for_chunk,\n",
    "                    \"chunk_right\": tile[1][\"tile_left\"] + number_pixel_columns_for_chunk,\n",
    "                }\n",
    "                number_of_chunks += 1\n",
    "\n",
    "                if True:\n",
    "                    # This implementations has a run time that is quadratic in the number of tiles that a slide has.  It\n",
    "                    # is too slow; we should make it faster.\n",
    "                    tiles = chunk[\"tiles\"] = {}\n",
    "                    subsequent_chunks = []\n",
    "                    for tile in tiles_as_sorted_list:\n",
    "                        if (\n",
    "                            tile[1][\"tile_top\"] + number_pixel_rows_for_tile <= chunk[\"chunk_bottom\"]\n",
    "                            and tile[1][\"tile_left\"] + number_pixel_columns_for_tile <= chunk[\"chunk_right\"]\n",
    "                            and tile[1][\"tile_left\"] >= chunk[\"chunk_left\"]\n",
    "                            and tile[1][\"tile_top\"] >= chunk[\"chunk_top\"]\n",
    "                        ):\n",
    "                            tiles[tile[0]] = tile[1]\n",
    "                        else:\n",
    "                            subsequent_chunks.append(tile)\n",
    "\n",
    "                else:\n",
    "                    # This implementations has a run time that is quadratic in the number of tiles that a slide has.  It\n",
    "                    # is even slower than the above.\n",
    "                    tiles = chunk[\"tiles\"] = {\n",
    "                        tile[0]: tile[1]\n",
    "                        for tile in tiles_as_sorted_list\n",
    "                        if tile[1][\"tile_top\"] + number_pixel_rows_for_tile <= chunk[\"chunk_bottom\"]\n",
    "                        and tile[1][\"tile_left\"] + number_pixel_columns_for_tile <= chunk[\"chunk_right\"]\n",
    "                        and tile[1][\"tile_left\"] >= chunk[\"chunk_left\"]\n",
    "                        and tile[1][\"tile_top\"] >= chunk[\"chunk_top\"]\n",
    "                    }\n",
    "                    subsequent_chunks = [\n",
    "                        tile\n",
    "                        for tile in tiles_as_sorted_list\n",
    "                        if not (\n",
    "                            tile[1][\"tile_top\"] + number_pixel_rows_for_tile <= chunk[\"chunk_bottom\"]\n",
    "                            and tile[1][\"tile_left\"] + number_pixel_columns_for_tile <= chunk[\"chunk_right\"]\n",
    "                            and tile[1][\"tile_left\"] >= chunk[\"chunk_left\"]\n",
    "                            and tile[1][\"tile_top\"] >= chunk[\"chunk_top\"]\n",
    "                        )\n",
    "                    ]\n",
    "\n",
    "                # Update the list of tiles that are not yet in chunks\n",
    "                tiles_as_sorted_list = subsequent_chunks\n",
    "\n",
    "                # Make the chunk as small as possible given the tiles that it must support.\n",
    "                # Note that this also ensures that the pixels that are read do not run over\n",
    "                # the bottom or right border of the slide (assuming that the tiles do not\n",
    "                # go over those borders).\n",
    "                chunk[\"chunk_top\"] = min([tile[\"tile_top\"] for tile in tiles.values()])\n",
    "                chunk[\"chunk_left\"] = min([tile[\"tile_left\"] for tile in tiles.values()])\n",
    "                chunk[\"chunk_bottom\"] = max([tile[\"tile_top\"] for tile in tiles.values()]) + number_pixel_rows_for_tile\n",
    "                chunk[\"chunk_right\"] = (\n",
    "                    max([tile[\"tile_left\"] for tile in tiles.values()]) + number_pixel_columns_for_tile\n",
    "                )\n",
    "\n",
    "    @tf.function\n",
    "    def _read_and_split_chunk_pixels(self, elem):\n",
    "        # Get chunk's pixel data from disk and load it into chunk_pixels_as_tensor\n",
    "        chunk_pixels_as_tensor = tf.py_function(\n",
    "            func=self._py_read_chunk_pixels,\n",
    "            inp=[\n",
    "                elem[\"chunk_top\"],\n",
    "                elem[\"chunk_left\"],\n",
    "                elem[\"chunk_bottom\"],\n",
    "                elem[\"chunk_right\"],\n",
    "                elem[\"filename\"],\n",
    "                elem[\"level\"],\n",
    "            ],\n",
    "            Tout=tf.uint8,\n",
    "        )\n",
    "        number_of_tiles = tf.size(elem[\"tiles_top\"])\n",
    "        tiles = tf.TensorArray(dtype=tf.uint8, size=number_of_tiles)\n",
    "\n",
    "        def condition(i, _):\n",
    "            return tf.less(i, number_of_tiles)\n",
    "\n",
    "        def body(i, tiles):\n",
    "            return (\n",
    "                i + 1,\n",
    "                tiles.write(\n",
    "                    i,\n",
    "                    tf.image.crop_to_bounding_box(\n",
    "                        chunk_pixels_as_tensor,\n",
    "                        tf.gather(elem[\"tiles_top\"], i) - elem[\"chunk_top\"],\n",
    "                        tf.gather(elem[\"tiles_left\"], i) - elem[\"chunk_left\"],\n",
    "                        elem[\"number_pixel_rows_for_tile\"],\n",
    "                        elem[\"number_pixel_columns_for_tile\"],\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        _, tiles = tf.while_loop(condition, body, [0, tiles])\n",
    "        tiles = tiles.stack()\n",
    "\n",
    "        response = {}\n",
    "        for key in elem.keys():\n",
    "            if key not in (\"tiles_top\", \"tiles_left\"):\n",
    "                response[key] = tf.repeat(elem[key], number_of_tiles)\n",
    "\n",
    "        response = {**response, \"tile_top\": elem[\"tiles_top\"], \"tile_left\": elem[\"tiles_left\"], \"tile_pixels\": tiles}\n",
    "        return response\n",
    "\n",
    "    def _py_read_chunk_pixels(self, chunk_top, chunk_left, chunk_bottom, chunk_right, filename, level=-1):\n",
    "        \"\"\"Read from disk all the pixel data for a specific chunk of the whole slide.\"\"\"\n",
    "\n",
    "        filename = filename.numpy().decode(\"utf-8\")\n",
    "        chunk_top = chunk_top.numpy()\n",
    "        chunk_left = chunk_left.numpy()\n",
    "        chunk_bottom = chunk_bottom.numpy()\n",
    "        chunk_right = chunk_right.numpy()\n",
    "        level = level.numpy()\n",
    "\n",
    "        if re.compile(r\"\\.svs$\").search(filename):\n",
    "            import openslide as os\n",
    "\n",
    "            os_obj = os.OpenSlide(filename)\n",
    "            chunk = np.array(\n",
    "                os_obj.read_region((chunk_left, chunk_top), level, (chunk_right - chunk_left, chunk_bottom - chunk_top))\n",
    "            )\n",
    "        else:\n",
    "            from PIL import Image\n",
    "\n",
    "            pil_obj = Image.open(filename)\n",
    "            chunk = np.asarray(pil_obj)[chunk_left:chunk_right, chunk_top:chunk_bottom, :]\n",
    "\n",
    "        # Do we want to support other than RGB and/or other than uint8?!!!\n",
    "        return tf.convert_to_tensor(chunk[..., :3], dtype=tf.uint8)\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "# Create a study and insert study-wide information\n",
    "my_study0 = {\"version\": \"version-1\"}\n",
    "my_study0[\"number_pixel_rows_for_tile\"] = 256\n",
    "my_study0[\"number_pixel_columns_for_tile\"] = 256\n",
    "my_slides = my_study0[\"slides\"] = {}\n",
    "\n",
    "# Add a slide to the study, including slide-wide information with it.\n",
    "my_slide0 = my_slides[\"Slide_0\"] = {}\n",
    "my_slide0[\"filename\"] = \"/tf/notebooks/histomics_detect/example/DCBT_10_CMYC.svs\"\n",
    "# my_slide0[\"filename\"] = \"/tf/notebooks/histomics_stream/example/TCGA-BH-A0BZ-01Z-00-DX1.45EB3E93-A871-49C6-9EAE-90D98AE01913.svs\"\n",
    "my_slide0[\"slide_name\"] = \"DCBT_10_CMYC\"\n",
    "my_slide0[\"slide_group\"] = \"DCBT_10\"\n",
    "my_slide0[\"number_pixel_rows_for_chunk\"] = 2048\n",
    "my_slide0[\"number_pixel_columns_for_chunk\"] = 2048\n",
    "\n",
    "# For each slide, find the appropriate resolution given the desired_magnification and magnification_tolerance.  In this\n",
    "# example, we use the same parameters for each slide, but this is not required generally.\n",
    "find_resolution_for_slide = FindResolutionForSlide(my_study0, desired_magnification=20, magnification_tolerance=0.02)\n",
    "for slide in my_study0[\"slides\"].values():\n",
    "    find_resolution_for_slide(slide)\n",
    "print(\"================================================================\")\n",
    "print(f\"my_study0 = {my_study0}\")\n",
    "\n",
    "# We are going to demonstrate several approaches to choosing tiles.  Each approach will start with its own copy of the\n",
    "# my_study0 that we have built so far.\n",
    "\n",
    "if True:\n",
    "    # Demonstrate TilesByGridAndMask without a mask\n",
    "    my_study_tiles_by_grid = copy.deepcopy(my_study0)\n",
    "    tiles_by_grid = TilesByGridAndMask(\n",
    "        my_study_tiles_by_grid,\n",
    "        number_pixel_overlap_rows_for_tile=32,\n",
    "        number_pixel_overlap_columns_for_tile=32,\n",
    "        randomly_select=100,\n",
    "    )\n",
    "    # We could apply this to a subset of the slides, but we will apply it to all slides in this example.\n",
    "    for slide in my_study_tiles_by_grid[\"slides\"].values():\n",
    "        tiles_by_grid(slide)\n",
    "    # print(\"================================================================\")\n",
    "    print(\"Finished with TilesByGrid\")\n",
    "    # print(f\"my_study_tiles_by_grid = {my_study_tiles_by_grid}\")\n",
    "\n",
    "if False:\n",
    "    # Demonstrate TilesByGridAndMask with a mask\n",
    "    my_study_tiles_by_grid_and_mask = copy.deepcopy(my_study0)\n",
    "    tiles_by_grid_and_mask = TilesByGridAndMask(\n",
    "        my_study_tiles_by_grid_and_mask,\n",
    "        number_pixel_overlap_rows_for_tile=0,\n",
    "        number_pixel_overlap_columns_for_tile=0,\n",
    "        mask_filename=\"/tf/notebooks/histomics_stream/example/TCGA-BH-A0BZ-01Z-00-DX1.45EB3E93-A871-49C6-9EAE-90D98AE01913-mask.png\",\n",
    "        randomly_select=100,\n",
    "    )\n",
    "    # We could apply this to a subset of the slides, but we will apply it to all slides in this example.\n",
    "    for slide in my_study_tiles_by_grid_and_mask[\"slides\"].values():\n",
    "        tiles_by_grid_and_mask(slide)\n",
    "    # print(\"================================================================\")\n",
    "    print(\"Finished with TilesByGridAndMask\")\n",
    "    # print(f\"my_study_tiles_by_grid_and_mask = {my_study_tiles_by_grid_and_mask}\")\n",
    "\n",
    "if True:\n",
    "    # Demonstrate TilesByList\n",
    "    my_study_tiles_by_list = copy.deepcopy(my_study0)\n",
    "    tiles_by_list = TilesByList(\n",
    "        my_study_tiles_by_list, randomly_select=5, tiles_dictionary=my_study_tiles_by_grid[\"slides\"][\"Slide_0\"][\"tiles\"]\n",
    "    )\n",
    "    # We could apply this to a subset of the slides, but we will apply it to all slides in this example.\n",
    "    for slide in my_study_tiles_by_list[\"slides\"].values():\n",
    "        tiles_by_list(slide)\n",
    "    # print(\"================================================================\")\n",
    "    print(\"Finished with TilesByList\")\n",
    "    # print(f\"my_study_tiles_by_list = {my_study_tiles_by_list}\")\n",
    "\n",
    "if True:\n",
    "    # Demonstrate TilesRandomly\n",
    "    my_study_tiles_randomly = copy.deepcopy(my_study0)\n",
    "    tiles_randomly = TilesRandomly(my_study_tiles_randomly, randomly_select=3)\n",
    "    # We could apply this to a subset of the slides, but we will apply it to all slides in this example.\n",
    "    for slide in my_study_tiles_randomly[\"slides\"].values():\n",
    "        tiles_randomly(slide)\n",
    "    # print(\"================================================================\")\n",
    "    print(\"Finished with TilesRandomly\")\n",
    "    # print(f\"my_study_tiles_randomly = {my_study_tiles_randomly}\")\n",
    "\n",
    "# We choose one of the above examples for further processing.\n",
    "my_study_of_tiles = my_study_tiles_by_grid\n",
    "# my_study_of_tiles = my_study_tiles_randomly\n",
    "\n",
    "create_tensorflow_dataset = CreateTensorFlowDataset()\n",
    "tiles = create_tensorflow_dataset(my_study_of_tiles)\n",
    "print(\"Finished with CreateTensorFlowDataset\")\n",
    "\n",
    "# print(\"================================================================\")\n",
    "# print(tiles)\n",
    "# print(\"================================================================\")\n",
    "# tf.print(tiles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Run with the tiles dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_map_options = {\n",
    "    \"num_parallel_calls\": tf.data.experimental.AUTOTUNE,\n",
    "    \"deterministic\": False,\n",
    "}\n",
    "\n",
    "# Convert pixel data to uint8\n",
    "tiles = tiles.map(lambda x, y: (tf.cast(x, tf.uint8), y), **dataset_map_options)\n",
    "\n",
    "# Run the model on the tiles\n",
    "tiles = tiles.map(lambda x, y: (x, model(x, tau=0.5, nms_iou=0.3), y), **dataset_map_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>Find a tile with many detections</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles2 = tiles\n",
    "tiles2 = tiles2.map(lambda x, p, y: (x, p, tf.shape(p)[0], y), **dataset_map_options)\n",
    "\n",
    "max_number_detections = -1\n",
    "number_tiles = 0\n",
    "for tile in tiles2:\n",
    "    number_tiles = number_tiles + 1\n",
    "    rgb, regressions, number_detections, _ = tile\n",
    "    if number_detections > max_number_detections:\n",
    "        tf.print(f\"New best: Tile #{number_tiles} has {number_detections} detections.\")\n",
    "        max_number_detections = number_detections\n",
    "        max_rgb = rgb\n",
    "        max_regressions = regressions\n",
    "    if max_number_detections ** 2 * number_tiles >= 10000:\n",
    "        break\n",
    "tf.print(f\"Examined {number_tiles} tiles in total.\")\n",
    "plot_inference(max_rgb, max_regressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Save and Load Model Weights</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save checkpoint\n",
    "model.save_weights(\"/tf/notebooks/histomics_detect/example/saved_model/\")\n",
    "\n",
    "# create dummy network for restore\n",
    "restored = FasterRCNN(rpnetwork, backbone, [width, height], anchor_px, rpn_lmbda)\n",
    "restored.load_weights(\"/tf/notebooks/histomics_detect/example/saved_model/\")\n",
    "\n",
    "# check that outputs are same\n",
    "assert tf.math.reduce_all(tf.math.equal(restored(rgb), model(rgb)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FasterRCNN_ragged.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
