{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import packages and install histomics_detect</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install histomics_detect\n",
    "!pip install -e /tf/notebooks/histomics_detect\n",
    "\n",
    "# install histomics_stream\n",
    "!pip install -e /tf/notebooks/histomics_stream\n",
    "\n",
    "# add to system path\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/tf/notebooks/histomics_detect/\")\n",
    "sys.path.append(\"/tf/notebooks/histomics_stream/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# import dataset related packages\n",
    "from histomics_detect.io import dataset\n",
    "from histomics_detect.augmentation import crop, flip, jitter, shrink\n",
    "from histomics_detect.visualization import plot_inference\n",
    "\n",
    "# import whole-slide image handling pipeline\n",
    "import histomics_stream as hs\n",
    "\n",
    "number_epochs = 50  # Set to a number smaller than 50 for speed during debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>Define dataset parameters and create datasets - DCC example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIvluD3u8-We"
   },
   "outputs": [],
   "source": [
    "# input data path\n",
    "path = \"/tf/notebooks/DCC/data/\"\n",
    "\n",
    "# training parameters\n",
    "train_tile = 224  # input image size\n",
    "min_area_thresh = 0.5  # % of object area that must be in random crop to be included\n",
    "width = tf.constant(train_tile, tf.int32)\n",
    "height = tf.constant(train_tile, tf.int32)\n",
    "min_area = tf.constant(min_area_thresh, tf.float32)\n",
    "\n",
    "# split dataset into training and validation\n",
    "cases = [\n",
    "    \"131458\",\n",
    "    \"91315_leica_at2_40x\",\n",
    "    \"135062\",\n",
    "    \"93094\",\n",
    "    \"131453\",\n",
    "    \"131450\",\n",
    "    \"135060\",\n",
    "    \"131463\",\n",
    "    \"131459\",\n",
    "    \"131440\",\n",
    "    \"131460\",\n",
    "    \"93096\",\n",
    "    \"131449\",\n",
    "    \"131457\",\n",
    "    \"131461\",\n",
    "    \"93098\",\n",
    "    \"131447\",\n",
    "    \"93092\",\n",
    "    \"131443\",\n",
    "    \"93095\",\n",
    "    \"131448\",\n",
    "    \"93099\",\n",
    "    \"91316_leica_at2_40x\",\n",
    "    \"131462\",\n",
    "    \"93091\",\n",
    "    \"135065\",\n",
    "    \"131446\",\n",
    "    \"131441\",\n",
    "    \"101626\",\n",
    "    \"93093\",\n",
    "    \"131454\",\n",
    "    \"93097\",\n",
    "    \"131445\",\n",
    "    \"131444\",\n",
    "    \"131456\",\n",
    "    \"93090\",\n",
    "]\n",
    "id = np.argsort(np.random.rand(len(cases) - 1))[0 : np.ceil(0.9 * len(cases)).astype(np.int32)]\n",
    "training = [cases[i] for i in id]\n",
    "validation = list(set(cases).difference(training))\n",
    "\n",
    "# define parser for filenames\n",
    "def parser(file):\n",
    "    name = os.path.splitext(file)[0]\n",
    "    case = name.split(\".\")[2]\n",
    "    roi = \".\".join([name.split(\".\")[1]] + name.split(\".\")[-3:])\n",
    "    return case, roi\n",
    "\n",
    "\n",
    "# generate training, validation datasets\n",
    "ds_train_roi = dataset(path, parser, parser, train_tile, training)\n",
    "ds_validation_roi = dataset(path, parser, parser, 0, validation)\n",
    "\n",
    "# build training dataset\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*crop(x, y, width, height, min_area_thresh), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*flip(x, y), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, jitter(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, shrink(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# build validation datasets\n",
    "ds_validation_roi = ds_validation_roi.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create and train detection model - DCC example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UR0XaVyYmbAr"
   },
   "outputs": [],
   "source": [
    "# import network generation and training packages\n",
    "from histomics_detect.networks.rpns import rpn\n",
    "from histomics_detect.models.faster_rcnn import FasterRCNN\n",
    "\n",
    "# choices for anchor sizes - all anchors 1:1 aspect ratio\n",
    "anchor_px = tf.constant([32, 64, 96], dtype=tf.int32)  # width/height of square anchors in pixels at input mag.\n",
    "\n",
    "# feature network parameters\n",
    "backbone_stride = 1  # strides in feature generation network convolution\n",
    "backbone_blocks = 14  # number of residual blocks to use in backbone\n",
    "backbone_dimension = 256  # number of features generated by rpn convolution\n",
    "\n",
    "# rpn network parameters\n",
    "rpn_kernel = [3]  # kernel size for rpn convolution\n",
    "rpn_act_conv = [\"relu\"]  # activation for rpn convolutional layers\n",
    "\n",
    "# anchor filtering parameters\n",
    "neg_max = 128  # maximum number of negative/positive anchors to keep in each roi\n",
    "pos_max = 128\n",
    "rpn_lmbda = 10.0  # weighting for rpn regression loss\n",
    "roialign_tiles = 3.0  # roialign - number of horizontal/vertical tiles in a proposal\n",
    "roialing_pool = 2.0  # roialign - number of horizontal/vertical samples in each tile\n",
    "\n",
    "# create backbone and rpn networks\n",
    "resnet50 = tf.keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=(train_tile, train_tile, 3),\n",
    "    pooling=None,\n",
    ")\n",
    "rpnetwork, backbone = rpn(\n",
    "    resnet50,\n",
    "    n_anchors=tf.size(anchor_px),\n",
    "    stride=backbone_stride,\n",
    "    blocks=backbone_blocks,\n",
    "    kernels=rpn_kernel,\n",
    "    dimensions=[backbone_dimension],\n",
    "    activations=rpn_act_conv,\n",
    ")\n",
    "\n",
    "# create FasterRCNN keras model\n",
    "model = FasterRCNN(rpnetwork, backbone, [width, height], anchor_px, rpn_lmbda)\n",
    "\n",
    "# compile FasterRCNN model with losses\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=[\n",
    "        tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        tf.keras.losses.Huber(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# fit FasterRCNN model\n",
    "model.fit(\n",
    "    x=ds_train_roi,\n",
    "    batch_size=1,\n",
    "    epochs=number_epochs,\n",
    "    verbose=1,\n",
    "    validation_data=ds_validation_roi,\n",
    "    validation_freq=number_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define dataset parameters and create datasets - DLBCL example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset related packages\n",
    "from histomics_detect.io import dataset, resize\n",
    "from histomics_detect.augmentation import crop, flip, jitter, shrink\n",
    "from histomics_detect.visualization import plot_inference\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# input data path\n",
    "path = \"/tf/notebooks/DLBCL/detection/\"\n",
    "\n",
    "# training parameters\n",
    "train_tile = 224  # input image size\n",
    "min_area_thresh = 0.5  # % of object area that must be in crop to be included\n",
    "width = tf.constant(train_tile, tf.int32)\n",
    "height = tf.constant(train_tile, tf.int32)\n",
    "min_area = tf.constant(min_area_thresh, tf.float32)\n",
    "\n",
    "# define filename parsers\n",
    "def png_parser(png):\n",
    "    file = os.path.splitext(png)[0]\n",
    "    case = file.split(\".\")[0]\n",
    "    roi = \".\".join(file.split(\".\")[1:])\n",
    "    return case, roi\n",
    "\n",
    "\n",
    "def csv_parser(csv):\n",
    "    file = os.path.splitext(csv)[0]\n",
    "    case = file.split(\".\")[0]\n",
    "    roi = \".\".join(file.split(\".\")[1:2] + file.split(\".\")[-3:])\n",
    "    return case, roi\n",
    "\n",
    "\n",
    "training = [\n",
    "    \"DCBT_2_CMYC\",\n",
    "    \"DCBT_3_CMYC\",\n",
    "    \"DCBT_5_CMYC\",\n",
    "    \"DCBT_9_CMYC\",\n",
    "    \"DCBT_10_CMYC\",\n",
    "    \"DCBT_12_CMYC\",\n",
    "    \"DCBT_14_CMYC\",\n",
    "    \"DCBT_18_CMYC\",\n",
    "    \"DCBT_19_CMYC\",\n",
    "    \"DCBT_20_CMYC\",\n",
    "    \"DCBT_21_CMYC\",\n",
    "    \"DCBT_22_CMYC\",\n",
    "]\n",
    "validation = [\n",
    "    \"DCBT_1_CMYC\",\n",
    "    \"DCBT_4_CMYC\",\n",
    "    \"DCBT_6_CMYC\",\n",
    "    \"DCBT_8_CMYC\",\n",
    "    \"DCBT_11_CMYC\",\n",
    "    \"DCBT_13_CMYC\",\n",
    "    \"DCBT_15_CMYC\",\n",
    "    \"DCBT_16_CMYC\",\n",
    "    \"DCBT_17_CMYC\",\n",
    "]\n",
    "\n",
    "\n",
    "# generate training, validation datasets\n",
    "ds_train_roi = dataset(path, png_parser, csv_parser, train_tile, training)\n",
    "ds_validation_roi = dataset(path, png_parser, csv_parser, 0, validation)\n",
    "\n",
    "# build training dataset\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*resize(x, y, 2.0), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*crop(x, y, width, height, min_area_thresh), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*flip(x, y), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, jitter(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, shrink(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# build validation datasets\n",
    "ds_validation_roi = ds_validation_roi.map(lambda x, y, z: (*resize(x, y, 2.0), z))\n",
    "ds_validation_roi = ds_validation_roi.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create and train detection model - DLBCL example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import network generation and training packages\n",
    "from histomics_detect.networks.rpns import rpn\n",
    "from histomics_detect.models.faster_rcnn import FasterRCNN\n",
    "\n",
    "# choices for anchor sizes - all anchors 1:1 aspect ratio\n",
    "anchor_px = tf.constant([32, 48, 64], dtype=tf.int32)  # width/height of square anchors in pixels at input mag.\n",
    "\n",
    "# feature network parameters\n",
    "backbone_stride = 1  # strides in feature generation network convolution\n",
    "backbone_blocks = 14  # number of residual blocks to use in backbone\n",
    "backbone_dimension = 256  # number of features generated by rpn convolution\n",
    "\n",
    "# rpn network parameters\n",
    "rpn_kernel = [3]  # kernel size for rpn convolution\n",
    "rpn_act_conv = [\"relu\"]  # activation for rpn convolutional layers\n",
    "\n",
    "# anchor filtering parameters\n",
    "neg_max = 128  # maximum number of negative/positive anchors to keep in each roi\n",
    "pos_max = 128\n",
    "rpn_lmbda = 10.0  # weighting for rpn regression loss\n",
    "roialign_tiles = 3.0  # roialign - number of horizontal/vertical tiles in a proposal\n",
    "roialing_pool = 2.0  # roialign - number of horizontal/vertical samples in each tile\n",
    "\n",
    "# create backbone and rpn networks\n",
    "resnet50 = tf.keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=(train_tile, train_tile, 3),\n",
    "    pooling=None,\n",
    ")\n",
    "rpnetwork, backbone = rpn(\n",
    "    resnet50,\n",
    "    n_anchors=tf.size(anchor_px),\n",
    "    stride=backbone_stride,\n",
    "    blocks=backbone_blocks,\n",
    "    kernels=rpn_kernel,\n",
    "    dimensions=[backbone_dimension],\n",
    "    activations=rpn_act_conv,\n",
    ")\n",
    "\n",
    "# create FasterRCNN keras model\n",
    "model = FasterRCNN(rpnetwork, backbone, [width, height], anchor_px, rpn_lmbda)\n",
    "\n",
    "# compile FasterRCNN model with losses\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=[\n",
    "        tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        tf.keras.losses.Huber(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# fit FasterRCNN model\n",
    "model.fit(\n",
    "    x=ds_train_roi,\n",
    "    batch_size=1,\n",
    "    epochs=number_epochs,\n",
    "    verbose=1,\n",
    "    validation_data=ds_validation_roi,\n",
    "    validation_freq=number_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Inference on a single image - model.call() </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and visualize thresholded, roialign outputs\n",
    "data = ds_validation_roi.shuffle(100).take(1).get_single_element()\n",
    "rgb = tf.cast(data[0], tf.uint8)\n",
    "regressions = model(rgb, tau=0.5, nms_iou=0.3)\n",
    "plot_inference(rgb, regressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Raw inference on a single image - model.raw() </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate raw rpn outputs\n",
    "objectness, boxes, features = model.raw(rgb)\n",
    "\n",
    "# threshold rpn proposals\n",
    "boxes_positive, objectness_positive, positive = model.threshold(boxes, objectness, model.tau)\n",
    "\n",
    "# perform non-max suppression on rpn positive predictions\n",
    "boxes_nms, objectness_nms, selected = model.nms(boxes_positive, objectness_positive, model.nms_iou)\n",
    "\n",
    "# generate roialign predictions for rpn positive predictions\n",
    "align_boxes = model.align(boxes_nms, features, model.field, model.pool, model.tiles)\n",
    "\n",
    "# apply thresholding, nms, and roialign\n",
    "plot_inference(rgb, align_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Batch inference using tf.data.Dataset.map </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping model using data.Dataset.map keeps outputs from different images separate\n",
    "map_output = ds_validation_roi.take(5).map(lambda x, y, z: (model(x), y, z))\n",
    "map_output = [element for element in map_output]\n",
    "\n",
    "# compare to using model.predict which merges the outputs from all images\n",
    "predict_output = model.predict(ds_validation_roi.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Batch evaluation - model.evaluate() </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance evaluation on multiple images from a tf.data.Dataset\n",
    "metrics = model.evaluate(ds_validation_roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>Read in one or more whole-slide images and create a tensorflow dataset of tiles.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if not device_name:\n",
    "    raise SystemError(\"GPU device not found\")\n",
    "print(\"Found GPU at: {}\".format(device_name))\n",
    "\n",
    "# Options for tensorflow dataset map operations\n",
    "dataset_map_options = {\n",
    "    \"num_parallel_calls\": tf.data.experimental.AUTOTUNE,\n",
    "    \"deterministic\": False,\n",
    "}\n",
    "\n",
    "# The list of whole-slide images to process\n",
    "all_wsi_images = [\"/tf/notebooks/histomics_detect/example/DCBT_10_CMYC.svs\"]\n",
    "print(f\"Image source = {all_wsi_images}\")\n",
    "\n",
    "# We will use a mask to determine which tiles of a slide to process.  In this example we will\n",
    "# build the mask name from the WSI file name, by inserting \"-mask\" and changing the file type to\n",
    "# \"png\", but generally any file name and any file type that we can read as an image will do.\n",
    "all_masks = [re.sub(r\"^(.*)\\.([^\\.]*)$\", r\"\\1-mask.png\", wsi) for wsi in all_wsi_images]\n",
    "# Or, instead use no masks\n",
    "all_masks = [\"\" for wsi in all_wsi_images]\n",
    "print(f\"all_masks = {all_masks}\")\n",
    "\n",
    "# Create a tensorflow dataset that knows something about our whole-slide images\n",
    "header = dict(\n",
    "    hs.ds.init.Header(\n",
    "        slides=\"DCBT_10_CMYC\",\n",
    "        filenames=all_wsi_images,\n",
    "        cases=\"DCBT_10\",\n",
    "        magnifications=20.0,\n",
    "        read_modes=\"tiled\",\n",
    "        mask_filenames=all_masks,\n",
    "    )\n",
    ")\n",
    "slides = tf.data.Dataset.from_tensor_slices(header)\n",
    "\n",
    "# For the desired magnification, find the best level stored in the image file, and its associated\n",
    "# factor, width, and height.\n",
    "compute_read_parameters = hs.dsm.wsi.ComputeReadParameters()\n",
    "slides = slides.map(compute_read_parameters, **dataset_map_options)\n",
    "\n",
    "# Specify size, overlap, etc. information about the tiles that we want to analyze.\n",
    "add_tile_description = hs.dsm.wsi.AddTileDescription(\n",
    "    tile_width=tf.constant(224, dtype=tf.int32),\n",
    "    tile_height=tf.constant(224, dtype=tf.int32),\n",
    "    overlap_width=tf.constant(0, dtype=tf.int32),\n",
    "    overlap_height=tf.constant(0, dtype=tf.int32),\n",
    "    chunk_width_factor=tf.constant(8, dtype=tf.int32),\n",
    "    chunk_height_factor=tf.constant(8, dtype=tf.int32),\n",
    ")\n",
    "slides = slides.map(add_tile_description, **dataset_map_options)\n",
    "\n",
    "# If there are any then read in the masks, one per slide, that specify tile selction.  If they are\n",
    "# not already then downsample (or upsample) the masks to be one pixel per tile.\n",
    "compute_resampled_mask = hs.dsm.wsi.ComputeResampledMask()\n",
    "slides = slides.map(compute_resampled_mask, **dataset_map_options)\n",
    "\n",
    "# Split each element (e.g. each slide) into a batch of multiple rows, one per chunk to be read.\n",
    "# Note that the width `cw` or height `ch` of a row (chunk) may decreased from the requested value if\n",
    "# a chunk is near the edge of an image.  Note that it is important to call `.unbatch()` when it is\n",
    "# desired that the chunks be not batched by slide.\n",
    "compute_chunk_positions = hs.dsm.wsi.ComputeChunkPositions()\n",
    "chunks = slides.map(compute_chunk_positions, **dataset_map_options).prefetch(tf.data.experimental.AUTOTUNE).unbatch()\n",
    "\n",
    "# Read and split the chunks into the tile size we want.  Note that it is important to call\n",
    "# `.unbatch()` when it is desired that the tiles be not batched by chunk.\n",
    "read_and_split_chunk = hs.dsm.chunk.ReadAndSplitChunk()\n",
    "tiles = chunks.map(read_and_split_chunk, **dataset_map_options).prefetch(tf.data.experimental.AUTOTUNE).unbatch()\n",
    "\n",
    "# Export the tile's pixel data from the dictionary to top level\n",
    "tiles = tiles.map(lambda elem: (elem.pop(\"tile\"), elem), **dataset_map_options)\n",
    "\n",
    "# Convert pixel data to uint8\n",
    "tiles = tiles.map(lambda x, y: (tf.cast(x, tf.uint8), y), **dataset_map_options)\n",
    "\n",
    "# Run the model on the tiles\n",
    "tiles = tiles.map(lambda x, y: (x, model(x, tau=0.5, nms_iou=0.3), y), **dataset_map_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>Find a tile with many detections</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles2 = tiles\n",
    "tiles2 = tiles2.map(lambda x, p, y: (x, p, tf.shape(p)[0], y), **dataset_map_options)\n",
    "tiles2 = tiles2.take(1000)\n",
    "\n",
    "max_number_detections = 0\n",
    "for tile in tiles2:\n",
    "    rgb, regressions, number_detections, _ = tile\n",
    "    if number_detections >= tf.maximum(1, max_number_detections):\n",
    "        max_number_detections = number_detections\n",
    "        tf.print(f\"Found number_detections = {number_detections}\")\n",
    "\n",
    "tf.print(f\"max_number_detections = {max_number_detections}\")\n",
    "\n",
    "for tile in tiles2:\n",
    "    rgb, regressions, number_detections, _ = tile\n",
    "    if number_detections == max_number_detections:\n",
    "        tf.print(f\"max_number_detections = {max_number_detections}\")\n",
    "        plot_inference(rgb, regressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Save and Load Model Weights</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save checkpoint\n",
    "model.save_weights(\"/tf/notebooks/histomics_detect/example/saved_model/\")\n",
    "\n",
    "# create dummy network for restore\n",
    "restored = FasterRCNN(rpnetwork, backbone, [width, height], anchor_px, rpn_lmbda)\n",
    "restored.load_weights(\"/tf/notebooks/histomics_detect/example/saved_model/\")\n",
    "\n",
    "# check that outputs are same\n",
    "assert tf.math.reduce_all(tf.math.equal(restored(rgb), model(rgb)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FasterRCNN_ragged.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
