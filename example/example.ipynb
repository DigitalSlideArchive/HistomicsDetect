{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import packages and install histomics_detect</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "#install histomics_detect\n",
    "!pip install -e /tf/notebooks/histomics_detect\n",
    "\n",
    "#add to system path\n",
    "sys.path.append('/tf/notebooks/histomics_detect/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define dataset parameters and create datasets - DCC example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIvluD3u8-We"
   },
   "outputs": [],
   "source": [
    "#import dataset related packages\n",
    "from histomics_detect.io import dataset, resize\n",
    "from histomics_detect.augmentation import crop, flip, jitter, shrink\n",
    "from histomics_detect.visualization import plot_inference\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#input data path\n",
    "path = '/tf/notebooks/DCC/data/'\n",
    "\n",
    "#training parameters\n",
    "train_tile = 224 #input image size\n",
    "min_area_thresh = 0.5 # % of object area that must be in random crop to be included\n",
    "width = tf.constant(train_tile, tf.int32)\n",
    "height = tf.constant(train_tile, tf.int32)\n",
    "min_area = tf.constant(min_area_thresh, tf.float32)\n",
    "\n",
    "#split dataset into training and validation\n",
    "cases = ['131458', '91315_leica_at2_40x', '135062', '93094',\n",
    "         '131453', '131450', '135060', '131463', '131459',\n",
    "         '131440', '131460', '93096', '131449', '131457',\n",
    "         '131461', '93098', '131447', '93092', '131443',\n",
    "         '93095', '131448', '93099', '91316_leica_at2_40x', '131462',\n",
    "         '93091', '135065', '131446', '131441', '101626',\n",
    "         '93093', '131454', '93097', '131445', '131444',\n",
    "         '131456', '93090']\n",
    "id = np.argsort(np.random.rand(len(cases)-1))[0:np.ceil(0.9*len(cases)).astype(np.int32)]\n",
    "training = [cases[i] for i in id]\n",
    "validation = list(set(cases).difference(training))\n",
    "\n",
    "#define parser for filenames\n",
    "def parser(file):\n",
    "    name = os.path.splitext(file)[0]\n",
    "    case = name.split('.')[2]\n",
    "    roi = '.'.join([name.split('.')[1]] + name.split('.')[-3:])\n",
    "    return case, roi\n",
    "\n",
    "#generate training, validation datasets\n",
    "ds_train_roi = dataset(path, parser, parser, train_tile, training)\n",
    "ds_validation_roi = dataset(path, parser, parser, 0, validation)\n",
    "\n",
    "#build training dataset\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*crop(x, y, width, height, \n",
    "                                                                 min_area_thresh), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*flip(x, y), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, jitter(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, shrink(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "#build validation datasets\n",
    "ds_validation_roi = ds_validation_roi.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create and train detection model - DCC example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UR0XaVyYmbAr"
   },
   "outputs": [],
   "source": [
    "#import network generation and training packages\n",
    "from histomics_detect.networks.rpns import rpn\n",
    "from histomics_detect.models.faster_rcnn import FasterRCNN\n",
    "\n",
    "#choices for anchor sizes - all anchors 1:1 aspect ratio\n",
    "anchor_px = tf.constant([32, 64, 96], dtype=tf.int32) #width/height of square anchors in pixels at input mag.\n",
    "\n",
    "#feature network parameters\n",
    "backbone_stride = 1 #strides in feature generation network convolution\n",
    "backbone_blocks = 14 #number of residual blocks to use in backbone\n",
    "backbone_dimension = 256 #number of features generated by rpn convolution\n",
    "\n",
    "#rpn network parameters\n",
    "rpn_kernel = [3] #kernel size for rpn convolution\n",
    "rpn_act_conv = ['relu'] #activation for rpn convolutional layers\n",
    "\n",
    "#anchor filtering parameters\n",
    "neg_max = 128 #maximum number of negative/positive anchors to keep in each roi\n",
    "pos_max = 128\n",
    "rpn_lmbda = 10.0 #weighting for rpn regression loss\n",
    "roialign_tiles = 3.0 #roialign - number of horizontal/vertical tiles in a proposal\n",
    "roialing_pool = 2.0 #roialign - number of horizontal/vertical samples in each tile\n",
    "\n",
    "#create backbone and rpn networks\n",
    "resnet50 = tf.keras.applications.ResNet50(\n",
    "    include_top=False, weights='imagenet', input_tensor=None,\n",
    "    input_shape=(train_tile, train_tile, 3), pooling=None)\n",
    "rpnetwork, backbone = rpn(resnet50, n_anchors=tf.size(anchor_px),\n",
    "                          stride=backbone_stride, blocks=backbone_blocks, \n",
    "                          kernels=rpn_kernel, dimensions=[backbone_dimension],\n",
    "                          activations=rpn_act_conv)\n",
    "\n",
    "#create FasterRCNN keras model\n",
    "model = FasterRCNN(rpnetwork, backbone, [width, height], anchor_px, rpn_lmbda)\n",
    "\n",
    "#compile FasterRCNN model with losses\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss=[tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                    tf.keras.losses.Huber()])\n",
    "\n",
    "#fit FasterRCNN model\n",
    "model.fit(x=ds_train_roi, batch_size=1, epochs=50, verbose=1,\n",
    "          validation_data=ds_validation_roi, validation_freq=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define dataset parameters and create datasets - DLBCL example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset related packages\n",
    "from histomics_detect.io import dataset\n",
    "from histomics_detect.augmentation import crop, flip, jitter, shrink\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#input data path\n",
    "path = '/tf/notebooks/DLBCL/detection/'\n",
    "\n",
    "#training parameters\n",
    "train_tile = 224 #input image size\n",
    "min_area_thresh = 0.5 # % of object area that must be in crop to be included\n",
    "width = tf.constant(train_tile, tf.int32)\n",
    "height = tf.constant(train_tile, tf.int32)\n",
    "min_area = tf.constant(min_area_thresh, tf.float32)\n",
    "\n",
    "#define filename parsers\n",
    "def png_parser(png):\n",
    "    file = os.path.splitext(png)[0]\n",
    "    case = file.split('.')[0]\n",
    "    roi = '.'.join(file.split('.')[1:])\n",
    "    return case, roi\n",
    "\n",
    "def csv_parser(csv):\n",
    "    file = os.path.splitext(csv)[0]    \n",
    "    case = file.split('.')[0]\n",
    "    roi = '.'.join(file.split('.')[1:2] + file.split('.')[-3:])\n",
    "    return case, roi\n",
    "\n",
    "training = ['DCBT_2_CMYC', 'DCBT_3_CMYC', 'DCBT_5_CMYC',\n",
    "            'DCBT_9_CMYC', 'DCBT_10_CMYC', 'DCBT_12_CMYC', \n",
    "            'DCBT_14_CMYC', 'DCBT_18_CMYC', 'DCBT_19_CMYC', \n",
    "            'DCBT_20_CMYC', 'DCBT_21_CMYC', 'DCBT_22_CMYC']\n",
    "validation = ['DCBT_1_CMYC', 'DCBT_4_CMYC', 'DCBT_6_CMYC',\n",
    "              'DCBT_7_CMYC', 'DCBT_8_CMYC', 'DCBT_11_CMYC',\n",
    "              'DCBT_13_CMYC', 'DCBT_15_CMYC', 'DCBT_16_CMYC',\n",
    "              'DCBT_17_CMYC']\n",
    "\n",
    "\n",
    "#generate training, validation datasets\n",
    "ds_train_roi = dataset(path, png_parser, csv_parser, train_tile, training)\n",
    "ds_validation_roi = dataset(path, png_parser, csv_parser, 0, validation)\n",
    "\n",
    "#build training dataset\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*resize(x, y, 2.0), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*crop(x, y, width, height, \n",
    "                                                                 min_area_thresh), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*flip(x, y), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, jitter(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, shrink(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "#build validation datasets\n",
    "ds_validation_roi = ds_validation_roi.map(lambda x, y, z: (*resize(x, y, 2.0), z))\n",
    "ds_validation_roi = ds_validation_roi.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create and train detection model - DLBCL example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import network generation and training packages\n",
    "from histomics_detect.networks.rpns import rpn\n",
    "from histomics_detect.models.faster_rcnn import FasterRCNN\n",
    "\n",
    "#choices for anchor sizes - all anchors 1:1 aspect ratio\n",
    "anchor_px = tf.constant([32, 64, 96], dtype=tf.int32) #width/height of square anchors in pixels at input mag.\n",
    "\n",
    "#feature network parameters\n",
    "backbone_stride = 1 #strides in feature generation network convolution\n",
    "backbone_blocks = 14 #number of residual blocks to use in backbone\n",
    "backbone_dimension = 256 #number of features generated by rpn convolution\n",
    "\n",
    "#rpn network parameters\n",
    "rpn_kernel = [3] #kernel size for rpn convolution\n",
    "rpn_act_conv = ['relu'] #activation for rpn convolutional layers\n",
    "\n",
    "#anchor filtering parameters\n",
    "neg_max = 128 #maximum number of negative/positive anchors to keep in each roi\n",
    "pos_max = 128\n",
    "rpn_lmbda = 10.0 #weighting for rpn regression loss\n",
    "roialign_tiles = 3.0 #roialign - number of horizontal/vertical tiles in a proposal\n",
    "roialing_pool = 2.0 #roialign - number of horizontal/vertical samples in each tile\n",
    "\n",
    "#create backbone and rpn networks\n",
    "resnet50 = tf.keras.applications.ResNet50(\n",
    "    include_top=False, weights='imagenet', input_tensor=None,\n",
    "    input_shape=(train_tile, train_tile, 3), pooling=None)\n",
    "rpnetwork, backbone = rpn(resnet50, n_anchors=tf.size(anchor_px),\n",
    "                          stride=backbone_stride, blocks=backbone_blocks, \n",
    "                          kernels=rpn_kernel, dimensions=[backbone_dimension],\n",
    "                          activations=rpn_act_conv)\n",
    "\n",
    "#create FasterRCNN keras model\n",
    "model = FasterRCNN(rpnetwork, backbone, [width, height], anchor_px, rpn_lmbda)\n",
    "\n",
    "#compile FasterRCNN model with losses\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss=[tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                    tf.keras.losses.Huber()])\n",
    "\n",
    "#fit FasterRCNN model\n",
    "model.fit(x=ds_train_roi, batch_size=1, epochs=50, verbose=1,\n",
    "          validation_data=ds_validation_roi, validation_freq=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Inference on a single image - model.call() </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "ONXkJrfcsE4w",
    "outputId": "72c5aca2-5256-4428-86f4-8e262e9d6414",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generate and visualize thresholded, roialign outputs\n",
    "data = list(ds_validation_roi.shuffle(100).take(1).as_numpy_iterator())[0]\n",
    "rgb = data[0]\n",
    "regressions = model(rgb, threshold=0.5, nms_iou=0.3)\n",
    "plot_inference(rgb, regressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Raw inference on a single image - model.raw() </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate and visualize raw rpn outputs\n",
    "_, regressions, _ = model.raw(rgb)\n",
    "plot_inference(rgb, regressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Batch inference using tf.data.Dataset.map </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping model using data.Dataset.map keeps outputs from different images separate \n",
    "map_output = list(ds_validation_roi.take(5).map(lambda x, y, z: (model(x), y, z)).as_numpy_iterator())\n",
    "\n",
    "#compare to using model.predict which merges the outputs from all images\n",
    "predict_output = model.predict(ds_validation_roi.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Batch evaluation - model.evaluate() </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance evaluation on multiple images from a tf.data.Dataset\n",
    "metrics = model.evaluate(ds_validation_roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Save and Load Model Weights</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = \"cpk\"\n",
    "model.save_weights(weight_path)\n",
    "model_reload = FasterRCNN(rpnetwork, backbone, [width, height], anchor_px, rpn_lmbda)\n",
    "model_reload.load_weights(weight_path)\n",
    "\n",
    "align_reg = model.fastrcnn(interpolated)\n",
    "align_reg_reload = model_reload.fastrcnn(interpolated)\n",
    "\n",
    "assert tf.reduce_sum(tf.cast(align_reg_reload == align_reg, tf.int32)) == tf.math.reduce_prod(tf.shape(align_reg))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FasterRCNN_ragged.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
