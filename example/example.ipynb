{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import packages and install histomics_detect</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "#install histomics_detect\n",
    "!pip install -e /tf/notebooks/histomics_detect\n",
    "\n",
    "#add to system path\n",
    "sys.path.append('/tf/notebooks/histomics_detect/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define dataset parameters and create datasets - DCC example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIvluD3u8-We"
   },
   "outputs": [],
   "source": [
    "#import dataset related packages\n",
    "from histomics_detect.io import dataset\n",
    "from histomics_detect.augmentation import crop, flip, jitter, shrink\n",
    "from histomics_detect.visualization import plot_inference\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#input data path\n",
    "path = '/tf/notebooks/DCC/data/'\n",
    "\n",
    "#training parameters\n",
    "train_tile = 224 #input image size\n",
    "min_area_thresh = 0.5 # % of object area that must be in random crop to be included\n",
    "width = tf.constant(train_tile, tf.int32)\n",
    "height = tf.constant(train_tile, tf.int32)\n",
    "min_area = tf.constant(min_area_thresh, tf.float32)\n",
    "\n",
    "#split dataset into training and validation\n",
    "cases = ['131458', '91315_leica_at2_40x', '135062', '93094',\n",
    "         '131453', '131450', '135060', '131463', '131459',\n",
    "         '131440', '131460', '93096', '131449', '131457',\n",
    "         '131461', '93098', '131447', '93092', '131443',\n",
    "         '93095', '131448', '93099', '91316_leica_at2_40x', '131462',\n",
    "         '93091', '135065', '131446', '131441', '101626',\n",
    "         '93093', '131454', '93097', '131445', '131444',\n",
    "         '131456', '93090']\n",
    "id = np.argsort(np.random.rand(len(cases)-1))[0:np.ceil(0.9*len(cases)).astype(np.int32)]\n",
    "training = [cases[i] for i in id]\n",
    "validation = list(set(cases).difference(training))\n",
    "\n",
    "#define parser for filenames\n",
    "def parser(file):\n",
    "    name = os.path.splitext(file)[0]\n",
    "    case = name.split('.')[2]\n",
    "    roi = '.'.join([name.split('.')[1]] + name.split('.')[-3:])\n",
    "    return case, roi\n",
    "\n",
    "#generate training, validation datasets\n",
    "ds_train_roi = dataset(path, parser, parser, train_tile, training)\n",
    "ds_validation_roi = dataset(path, parser, parser, 0, validation)\n",
    "\n",
    "#build training dataset\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*crop(x, y, width, height, \n",
    "                                                                 min_area_thresh), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*flip(x, y), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, jitter(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, shrink(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "#build validation datasets\n",
    "ds_validation_roi = ds_validation_roi.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create and train detection model - DCC example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UR0XaVyYmbAr"
   },
   "outputs": [],
   "source": [
    "#import network generation and training packages\n",
    "from histomics_detect.networks.backbones import pretrained, residual\n",
    "from histomics_detect.networks.rpns import rpn\n",
    "from histomics_detect.networks.fast_rcnn import fast_rcnn\n",
    "from histomics_detect.models.faster_rcnn import FasterRCNN\n",
    "\n",
    "#choices for anchor sizes - all anchors 1:1 aspect ratio\n",
    "anchor_sizes = [32, 64, 96] #width/height of square anchors in pixels at input mag.\n",
    "\n",
    "#feature network parameters\n",
    "backbone_args = {'name': 'resnet50v2',\n",
    "                 'stride': 1, #stride (pixels) in first convolution of backbone\n",
    "                 'blocks': 14} #number of residual blocks to use in backbone\n",
    "\n",
    "#rpn network parameters\n",
    "rpn_args = {'kernels': [3], #kernel sizes (receptive fields) for rpn convolutions\n",
    "            'dimensions': [256], #number of kernels per layer\n",
    "            'activations': ['relu']} #activation for rpn convolutions\n",
    "\n",
    "#fast-rcnn network parameters\n",
    "frcnn_args = {'units': [4096, 4096], #number of units in fast-rcnn dense layers\n",
    "              'activations': ['relu', 'relu'], #activations for each dense layer\n",
    "              'pool': 2, #number of tiles to pool during roialign\n",
    "              'tiles': 3} #number of tiles to split regressed boxes into during roialign\n",
    "\n",
    "#training parameters\n",
    "train_args = {'train_shape': (224, 224, 3), #shape of training instances\n",
    "              'max_anchors': 256, #maximum number of negative anchors to sample per epoch\n",
    "              'np_ratio': 0.5, #largest ratio of negative : positive anchors per batch\n",
    "              'lmbda': 10.0} #weighting factor for region-proposal network regression loss\n",
    "\n",
    "#validation parameters\n",
    "validation_args = {'tau': 0.5, #objectness threshold used to call positive anchors at inference\n",
    "                   'nms_iou': 0.3, #min nms threshold used to filter overlapping objects\n",
    "                   'tpr_iou': 0.5, #single iou threshold used to calculate tpr, fnr during validation\n",
    "                   'margin': 32, #margin in pixels around image edge to exclude from validation\n",
    "                   'ap_ious': [0.25, 0.5, 0.75], #ious thresholds to use in evaluating mAP\n",
    "                   'ap_delta': 0.1} #precision step size for calculating mAP\n",
    "\n",
    "#create FasterRCNN keras model\n",
    "model = FasterRCNN(backbone_args, rpn_args, frcnn_args, train_args, validation_args, anchor_sizes)\n",
    "\n",
    "#compile FasterRCNN model with losses\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss=[tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                    tf.keras.losses.Huber()])\n",
    "\n",
    "#fit FasterRCNN model\n",
    "model.fit(x=ds_train_roi, batch_size=1, epochs=50, verbose=1,\n",
    "          validation_data=ds_validation_roi, validation_freq=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define dataset parameters and create datasets - DLBCL example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset related packages\n",
    "from histomics_detect.io import dataset, resize\n",
    "from histomics_detect.augmentation import crop, flip, jitter, shrink\n",
    "from histomics_detect.visualization import plot_inference\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#input data path\n",
    "path = '/tf/notebooks/DLBCL/detection/'\n",
    "\n",
    "#training parameters\n",
    "train_tile = 224 #input image size\n",
    "min_area_thresh = 0.5 # % of object area that must be in crop to be included\n",
    "width = tf.constant(train_tile, tf.int32)\n",
    "height = tf.constant(train_tile, tf.int32)\n",
    "min_area = tf.constant(min_area_thresh, tf.float32)\n",
    "\n",
    "#define filename parsers\n",
    "def png_parser(png):\n",
    "    file = os.path.splitext(png)[0]\n",
    "    case = file.split('.')[0]\n",
    "    roi = '.'.join(file.split('.')[1:])\n",
    "    return case, roi\n",
    "\n",
    "def csv_parser(csv):\n",
    "    file = os.path.splitext(csv)[0]    \n",
    "    case = file.split('.')[0]\n",
    "    roi = '.'.join(file.split('.')[1:2] + file.split('.')[-3:])\n",
    "    return case, roi\n",
    "\n",
    "training = ['DCBT_2_CMYC', 'DCBT_3_CMYC', 'DCBT_5_CMYC',\n",
    "            'DCBT_9_CMYC', 'DCBT_10_CMYC', 'DCBT_12_CMYC', \n",
    "            'DCBT_14_CMYC', 'DCBT_18_CMYC', 'DCBT_19_CMYC', \n",
    "            'DCBT_20_CMYC', 'DCBT_21_CMYC', 'DCBT_22_CMYC']\n",
    "validation = ['DCBT_1_CMYC', 'DCBT_4_CMYC', 'DCBT_6_CMYC',\n",
    "              'DCBT_8_CMYC', 'DCBT_11_CMYC',\n",
    "              'DCBT_13_CMYC', 'DCBT_15_CMYC', 'DCBT_16_CMYC',\n",
    "              'DCBT_17_CMYC']\n",
    "\n",
    "\n",
    "#generate training, validation datasets\n",
    "ds_train_roi = dataset(path, png_parser, csv_parser, train_tile, training)\n",
    "ds_validation_roi = dataset(path, png_parser, csv_parser, 0, validation)\n",
    "\n",
    "#build training dataset\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*resize(x, y, 2.0), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*crop(x, y, width, height, \n",
    "                                                                 min_area_thresh), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*flip(x, y), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, jitter(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, shrink(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "#build validation datasets\n",
    "ds_validation_roi = ds_validation_roi.map(lambda x, y, z: (*resize(x, y, 2.0), z))\n",
    "ds_validation_roi = ds_validation_roi.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create and train detection model - DLBCL example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import network generation and training packages\n",
    "from histomics_detect.networks.rpns import rpn\n",
    "from histomics_detect.models.faster_rcnn import FasterRCNN\n",
    "\n",
    "#choices for anchor sizes - all anchors 1:1 aspect ratio\n",
    "anchor_px = tf.constant([32, 48, 64], dtype=tf.int32) #width/height of square anchors in pixels at input mag.\n",
    "\n",
    "#feature network parameters\n",
    "backbone_stride = 1 #strides in feature generation network convolution\n",
    "backbone_blocks = 14 #number of residual blocks to use in backbone\n",
    "backbone_dimension = 256 #number of features generated by rpn convolution\n",
    "\n",
    "#rpn network parameters\n",
    "rpn_kernel = [3] #kernel size for rpn convolution\n",
    "rpn_act_conv = ['relu'] #activation for rpn convolutional layers\n",
    "\n",
    "#anchor filtering parameters\n",
    "neg_max = 128 #maximum number of negative/positive anchors to keep in each roi\n",
    "pos_max = 128\n",
    "rpn_lmbda = 10.0 #weighting for rpn regression loss\n",
    "roialign_tiles = 3.0 #roialign - number of horizontal/vertical tiles in a proposal\n",
    "roialing_pool = 2.0 #roialign - number of horizontal/vertical samples in each tile\n",
    "\n",
    "#create backbone and rpn networks\n",
    "resnet50 = tf.keras.applications.ResNet50(\n",
    "    include_top=False, weights='imagenet', input_tensor=None,\n",
    "    input_shape=(train_tile, train_tile, 3), pooling=None)\n",
    "rpnetwork, backbone = rpn(resnet50, n_anchors=tf.size(anchor_px),\n",
    "                          stride=backbone_stride, blocks=backbone_blocks, \n",
    "                          kernels=rpn_kernel, dimensions=[backbone_dimension],\n",
    "                          activations=rpn_act_conv)\n",
    "\n",
    "#create FasterRCNN keras model\n",
    "model = FasterRCNN(rpnetwork, backbone, [width, height], anchor_px, rpn_lmbda)\n",
    "\n",
    "#compile FasterRCNN model with losses\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss=[tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                    tf.keras.losses.Huber()])\n",
    "\n",
    "#fit FasterRCNN model\n",
    "model.fit(x=ds_train_roi, batch_size=1, epochs=50, verbose=1,\n",
    "          validation_data=ds_validation_roi, validation_freq=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Inference on a single image - model.call() </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate and visualize thresholded, roialign outputs\n",
    "data = ds_validation_roi.shuffle(100).take(1).get_single_element()\n",
    "rgb = tf.cast(data[0], tf.uint8)\n",
    "regressions = model(rgb, tau=0.5, nms_iou=0.3)\n",
    "plot_inference(rgb, regressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Raw inference on a single image - model.raw() </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate raw rpn outputs\n",
    "objectness, boxes, features = model.raw(rgb)\n",
    "\n",
    "#threshold rpn proposals\n",
    "boxes_positive, objectness_positive, positive = model.threshold(boxes, objectness, model.tau)\n",
    "        \n",
    "#perform non-max suppression on rpn positive predictions\n",
    "boxes_nms, objectness_nms, selected = model.nms(boxes_positive, objectness_positive, model.nms_iou)\n",
    "        \n",
    "#generate roialign predictions for rpn positive predictions\n",
    "align_boxes = model.align(boxes_nms, features, model.field, model.pool, model.tiles)\n",
    "\n",
    "#apply thresholding, nms, and roialign\n",
    "plot_inference(rgb, align_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Batch inference using tf.data.Dataset.map </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping model using data.Dataset.map keeps outputs from different images separate \n",
    "map_output = ds_validation_roi.take(5).map(lambda x, y, z: (model(x), y, z))\n",
    "map_output = [element for element in map_output]\n",
    "\n",
    "#compare to using model.predict which merges the outputs from all images\n",
    "predict_output = model.predict(ds_validation_roi.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Batch evaluation - model.evaluate() </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance evaluation on multiple images from a tf.data.Dataset\n",
    "metrics = model.evaluate(ds_validation_roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Save and Load as Keras.Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute model output shape to prepare for saving - this shape can be changed after loading\n",
    "model.compute_output_shape([224, 224, 3])\n",
    "\n",
    "#save checkpoint\n",
    "model.save('as_keras')\n",
    "\n",
    "#load model\n",
    "restored = tf.keras.models.load_model('as_keras', custom_objects={'FasterRCNN': FasterRCNN}, compile=True)\n",
    "\n",
    "#check that outputs are same\n",
    "assert tf.math.reduce_all(tf.math.equal(restored(rgb), model(rgb)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FasterRCNN_ragged.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
