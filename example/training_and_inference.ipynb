{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train and validate a Faster-RCNN detection model</h2>\n",
    "\n",
    "This notebook demonstrates how to train and validate a detection model using histomics_detect. We first illustrate how to formulate training and validation datasets, then use these with the keras. Different options for inference and training are demonstrated, as well as visualization of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "# install histomics_detect\n",
    "!pip install -e /tf/notebooks/histomics_detect\n",
    "\n",
    "# add to system path\n",
    "sys.path.append(\"/tf/notebooks/histomics_detect/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Build tf.data.Datasets for training and validation</h2>\n",
    "\n",
    "Download a dataset consisting of training and testing folders. Each sample consists of a paired .png and .csv file defining the locations of objects in the image. A parser function is defined to interpret information like the case and lab identifier from the dataset files for matching file pairs. During training, input images are randomly cropped on the fly to have uniform size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset related packages\n",
    "from histomics_detect.io import dataset\n",
    "from histomics_detect.augmentation import crop, flip, jitter, shrink\n",
    "import numpy as np\n",
    "import os\n",
    "import pooch\n",
    "\n",
    "# training parameters\n",
    "anchor_sizes = [24, 48, 64]  # width/height of square 1:1 anchors in pixels at input mag.\n",
    "train_tile = 224  # input image size\n",
    "min_area_thresh = 0.5  # % of object area that must be in random crop to be included\n",
    "width = tf.constant(train_tile, tf.int32)\n",
    "height = tf.constant(train_tile, tf.int32)\n",
    "min_area = tf.constant(min_area_thresh, tf.float32)\n",
    "\n",
    "# download data and unzip - pooch returns a list of all files in the archive\n",
    "path = pooch.retrieve(\n",
    "    url=\"https://northwestern.box.com/shared/static/m5n9zqnyoxtwb0xc5a08tg2ubmd5uowk\",\n",
    "    known_hash=\"152dbc5711b3c20adc52abd775072b6607e83572aed71befe9d7609131581e61\",\n",
    "    path=str(pooch.os_cache(\"pooch\")) + os.sep + \"data\",\n",
    "    processor=pooch.Unzip(),\n",
    ")\n",
    "\n",
    "# parse training and testing paths\n",
    "sep = os.path.sep\n",
    "root = sep.join(os.path.split(path[0])[0].split(sep)[0:-1]) + sep\n",
    "\n",
    "# define filename parser for dataset generation\n",
    "def parser(file):\n",
    "    roi = file.split(\".\")[0]\n",
    "    lab = file.split(\"_\")[0].split(\"-\")[1]\n",
    "    return roi, lab\n",
    "\n",
    "\n",
    "# generate training, validation datasets\n",
    "ds_train = dataset(root + \"train\" + sep, parser, parser, train_tile)\n",
    "ds_validate = dataset(root + \"test\" + sep, parser, parser, 0)\n",
    "\n",
    "# build training dataset\n",
    "ds_train = ds_train.map(lambda x, y, z: (*crop(x, y, width, height, min_area_thresh), z))\n",
    "ds_train = ds_train.map(lambda x, y, z: (*flip(x, y), z))\n",
    "ds_train = ds_train.map(lambda x, y, z: (x, jitter(y, 0.05), z))\n",
    "ds_train = ds_train.map(lambda x, y, z: (x, shrink(y, 0.05), z))\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# build validation datasets\n",
    "ds_validate = ds_validate.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create and train detection model</h2>\n",
    "\n",
    "Generate a faster-RCNN keras model using default hyperparameters. Assign losses, an optimizer, and fit the model. Basic performance metrics of the region proposal classifier and the box regressors are displayed during training epochs. In validation epochs more extensive metrics including mean average precision are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UR0XaVyYmbAr"
   },
   "outputs": [],
   "source": [
    "# import network generation and training packages\n",
    "from histomics_detect.models.faster_rcnn import FasterRCNN, faster_rcnn_config\n",
    "\n",
    "# get default network configurations\n",
    "backbone_args, rpn_args, frcnn_args, train_args, validation_args = faster_rcnn_config()\n",
    "\n",
    "# lower non-max suppression iou\n",
    "validation_args[\"nms_iou\"] = 0.2\n",
    "\n",
    "# create FasterRCNN keras model\n",
    "model = FasterRCNN(backbone_args, rpn_args, frcnn_args, train_args, validation_args, anchor_sizes)\n",
    "\n",
    "# compile FasterRCNN model with losses\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=[tf.keras.losses.Hinge(), tf.keras.losses.Huber()],\n",
    ")\n",
    "\n",
    "# fit FasterRCNN model\n",
    "model.fit(x=ds_train, batch_size=1, epochs=40, verbose=1, validation_data=ds_validate, validation_freq=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Inference using model.call() </h2>\n",
    "\n",
    "Calling the model directly on an image applies all steps including roialign and non-max suppression. Arguments can be passed to control the sensitivities of the region proposal and the non-maximum suppression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from histomics_detect.visualization import plot_inference\n",
    "\n",
    "# generate and visualize thresholded, roialign outputs\n",
    "data = ds_validate.shuffle(100).take(1).get_single_element()\n",
    "rgb = tf.cast(data[0], tf.uint8)\n",
    "regressions = model(rgb, tau=0.5, nms_iou=0.1)\n",
    "plot_inference(rgb, regressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pass a margin parameter to clear the border </h2>\n",
    "\n",
    "You can clear predictions at the border by passing a margin parameter to call. This is helpful when performing inference on a tiled version of a whole-slide image and stitching results from overlapping tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat call providing margin parameter\n",
    "regressions = model(rgb, tau=0.5, nms_iou=0.3, margin=32)\n",
    "plot_inference(rgb, regressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Generating intermediate outputs using model.raw()</h2>\n",
    "\n",
    "Outputs from the region-proposal network can be obtained by calling model.raw. These proposals can be further processed using custom functions or the provided methods for objectness thresholding, non-max suppression, or roialign. These outputs can also be used for test time augmentation where multiple inferences are aggregated prior to non-max suppression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate raw rpn outputs\n",
    "objectness, boxes, features = model.raw(rgb)\n",
    "\n",
    "# threshold rpn proposals\n",
    "boxes_positive, objectness_positive, positive = model.threshold(boxes, objectness, model.tau)\n",
    "\n",
    "# perform non-max suppression on rpn positive predictions\n",
    "boxes_nms, objectness_nms, selected = model.nms(boxes_positive, objectness_positive, 0.3)\n",
    "\n",
    "# generate roialign predictions for rpn positive predictions\n",
    "align_boxes = model.align(boxes_nms, features, model.field, model.pool, model.tiles)\n",
    "\n",
    "# apply thresholding, nms, and roialign\n",
    "plot_inference(rgb, align_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Batch inference - performance comparison</h2>\n",
    "\n",
    "Using .predict is much faster than calling the model() in a list comprehension but combines all results in a single array. We can keep the outputs separated by wrapping the model to add the index of the input sequence to the results. This also allows other metadata to be passed through and captured as outputs (in this case the input image name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# sample validation dataset\n",
    "trial_ds = ds_validate.take(10).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# mapping model using data.Dataset.map keeps outputs from different images separate\n",
    "start = time.time()\n",
    "map_output = [element for element in trial_ds.map(lambda x, y, z: (model(x), y, z))]\n",
    "print(\"dataset.map \" + str(time.time() - start) + \" seconds.\")\n",
    "\n",
    "# compare to using model.predict which merges the outputs from all images\n",
    "start = time.time()\n",
    "predict_output = model.predict(trial_ds)\n",
    "print(\".predict \" + str(time.time() - start) + \" seconds.\")\n",
    "\n",
    "# examine predict output\n",
    "print(\".predict output: \" + str(tf.shape(predict_output)))\n",
    "\n",
    "# define passthrough model\n",
    "class WrappedModel(tf.keras.Model):\n",
    "    def __init__(self, model, *args, **kwargs):\n",
    "        super(WrappedModel, self).__init__(*args, **kwargs)\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        boxes = self.model(inputs[0], *args, **kwargs)\n",
    "        index = tf.cast(inputs[3], tf.float32) * tf.ones((tf.shape(boxes)[0], 1))\n",
    "        return (tf.concat([boxes, index], 1), inputs[1], inputs[2])\n",
    "\n",
    "\n",
    "# wrap\n",
    "wrapped = WrappedModel(model)\n",
    "\n",
    "# combine model inputs with a tile index value\n",
    "index_ds = tf.data.Dataset.range(len(trial_ds))\n",
    "trial_ds = tf.data.Dataset.zip((trial_ds, index_ds))\n",
    "trial_ds = trial_ds.map(lambda x, y: ((x[0], x[1], x[2], y), None, None))\n",
    "\n",
    "# generate indexed predictions\n",
    "start = time.time()\n",
    "indexed_output = wrapped.predict(trial_ds)\n",
    "print(\"wrapped .predict \" + str(time.time() - start) + \" seconds.\")\n",
    "\n",
    "# tile index column is added to predictions\n",
    "print(indexed_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Save and load as Keras.Model</h2>\n",
    "\n",
    "Saving as a keras model allows the restored model to be trained for additional cycles, and preserved access to keras functions for inference. Parallel inference with a keras model is also easier than with other formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute model output shape to trigger build - this shape can be changed after loading\n",
    "model.compute_output_shape([224, 224, 3])\n",
    "\n",
    "# save keras model\n",
    "model.save(\"tcga_brca_model\")\n",
    "\n",
    "# load model\n",
    "restored = tf.keras.models.load_model(\"tcga_brca_model\", custom_objects={\"FasterRCNN\": FasterRCNN})\n",
    "\n",
    "# check that outputs are same\n",
    "assert tf.math.reduce_all(tf.math.equal(restored(rgb), model(rgb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Save and load as SavedModel</h2>\n",
    "\n",
    "The model can also be saved and loaded in the SavedModel format for use with TensorFlow or NVIDIA inference servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute model output shape to trigger build - this shape can be changed after loading\n",
    "model.compute_output_shape([224, 224, 3])\n",
    "\n",
    "# save SavedModel\n",
    "tf.saved_model.save(model, \"tcga_brca_model\")\n",
    "\n",
    "# load model\n",
    "restored = tf.saved_model.load(\"tcga_brca_model\")\n",
    "inference = restored.signatures[\"serving_default\"]\n",
    "\n",
    "# check that outputs are same within a tolerance\n",
    "tf.debugging.assert_near(inference(tf.cast(rgb, tf.float32))[\"output_1\"], model(rgb), 1e-6)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FasterRCNN_ragged.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
