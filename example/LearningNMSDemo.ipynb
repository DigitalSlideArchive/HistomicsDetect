{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LearningNMS Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import of Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e histomics_detect/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as kb\n",
    "# import tensorflow_addons as tfa\n",
    "\n",
    "import yaml\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from abc import ABC\n",
    "\n",
    "sys.path.append(\"/tf/notebooks/histomics_detect/\")\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from histomics_detect.anchors import create_anchors\n",
    "from histomics_detect.networks import field_size, residual_backbone, transfer_layers, rpn\n",
    "from histomics_detect.roialign import roialign\n",
    "from histomics_detect.metrics import iou, greedy_iou_mapping as greedy_iou\n",
    "from histomics_detect.models import FasterRCNN\n",
    "from histomics_detect.models import LearningNMS\n",
    "from histomics_detect.models.faster_rcnn import map_outputs\n",
    "from histomics_detect.io import resize\n",
    "from histomics_detect.augmentation import flip, crop\n",
    "from histomics_detect.boxes import unparameterize, filter_edge_boxes, parameterize\n",
    "from histomics_detect.visualization.visualization import _plot_boxes\n",
    "from histomics_detect.visualization.lnms_visualization import plot_inference, run_plot\n",
    "from histomics_detect.models.experiment_utils import run_experiments\n",
    "\n",
    "from histomics_detect.models.block_model import BlockModel\n",
    "from histomics_detect.models.compression_network import CompressionNetwork\n",
    "from histomics_detect.models.lnms_model import LearningNMS\n",
    "from histomics_detect.boxes.match import cluster_assignment\n",
    "from histomics_detect.models.lnms_loss import normal_loss, clustering_loss, paper_loss, xor_loss, normal_clustering_loss, calculate_labels, _pos_neg_loss_calculation\n",
    "from histomics_detect.metrics.lnms import tf_linear_sum_assignment\n",
    "from histomics_detect.models.model_utils import extract_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset related packages\n",
    "from histomics_detect.io import dataset, resize\n",
    "from histomics_detect.augmentation import crop, flip, jitter, shrink\n",
    "# from histomics_detect.visualization import plot_inference\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#input data path\n",
    "path = '/tf/notebooks/data/DLBCL/'\n",
    "\n",
    "factor = 3\n",
    "#training parameters\n",
    "train_tile = 224 #input image size\n",
    "min_area_thresh = 0.5 # % of object area that must be in crop to be included\n",
    "width = tf.constant(train_tile, tf.int32)\n",
    "height = tf.constant(train_tile, tf.int32)\n",
    "min_area = tf.constant(min_area_thresh, tf.float32)\n",
    "\n",
    "#define filename parsers\n",
    "def png_parser(png):\n",
    "    file = os.path.splitext(png)[0]\n",
    "    case = file.split('.')[0]\n",
    "    roi = '.'.join(file.split('.')[1:])\n",
    "    return case, roi\n",
    "\n",
    "def csv_parser(csv):\n",
    "    file = os.path.splitext(csv)[0]    \n",
    "    case = file.split('.')[0]\n",
    "    roi = '.'.join(file.split('.')[1:2] + file.split('.')[-3:])\n",
    "    return case, roi\n",
    "\n",
    "training = ['DCBT_2_CMYC', 'DCBT_3_CMYC', 'DCBT_5_CMYC',\n",
    "            'DCBT_9_CMYC', 'DCBT_10_CMYC', 'DCBT_12_CMYC', \n",
    "            'DCBT_14_CMYC', 'DCBT_18_CMYC', 'DCBT_19_CMYC', \n",
    "            'DCBT_20_CMYC', 'DCBT_21_CMYC', 'DCBT_22_CMYC']\n",
    "# training = ['DCBT_10_CMYC']\n",
    "validation = ['DCBT_1_CMYC', 'DCBT_4_CMYC', 'DCBT_6_CMYC',\n",
    "              'DCBT_8_CMYC', 'DCBT_11_CMYC',\n",
    "              'DCBT_13_CMYC', 'DCBT_15_CMYC', 'DCBT_16_CMYC',\n",
    "              'DCBT_17_CMYC']\n",
    "\n",
    "\n",
    "#generate training, validation datasets\n",
    "ds_train_roi = dataset(path, png_parser, csv_parser, train_tile, training)\n",
    "ds_validation_roi = dataset(path, png_parser, csv_parser, 0, validation)\n",
    "\n",
    "#build training dataset\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*resize(x, y, 1.0*factor), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*crop(x, y, width, height, \n",
    "                                                                 min_area_thresh), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (*flip(x, y), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, jitter(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.map(lambda x, y, z: (x, shrink(y, 0.05), z))\n",
    "ds_train_roi = ds_train_roi.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "#build validation datasets\n",
    "ds_validation_roi = ds_validation_roi.map(lambda x, y, z: (*resize(x, y, 1.0*factor), z))\n",
    "ds_validation_roi = ds_validation_roi.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Faster R-CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model ------------------------------------------------------------------------------------------\n",
    "\n",
    "#import network generation and training packages\n",
    "from histomics_detect.networks.rpns import rpn\n",
    "from histomics_detect.models.faster_rcnn import FasterRCNN\n",
    "\n",
    "#choices for anchor sizes - all anchors 1:1 aspect ratio\n",
    "anchor_px = tf.constant([32, 48, 64, 76, 96, 108], dtype=tf.int32) #width/height of square anchors in pixels at input mag.\n",
    "anchor_px = tf.constant([32, 48, 64], dtype=tf.int32)\n",
    "\n",
    "\n",
    "#feature network parameters\n",
    "backbone_stride = 1 #strides in feature generation network convolution\n",
    "backbone_blocks = 14 #number of residual blocks to use in backbone\n",
    "backbone_dimension = 256 #number of features generated by rpn convolution\n",
    "\n",
    "#rpn network parameters\n",
    "rpn_kernel = [3] #kernel size for rpn convolution\n",
    "rpn_act_conv = ['relu'] #activation for rpn convolutional layers\n",
    "\n",
    "#anchor filtering parameters\n",
    "neg_max = 128 #maximum number of negative/positive anchors to keep in each roi\n",
    "pos_max = 128\n",
    "rpn_lmbda = 10.0 #weighting for rpn regression loss\n",
    "roialign_tiles = 3.0 #roialign - number of horizontal/vertical tiles in a proposal\n",
    "roialing_pool = 2.0 #roialign - number of horizontal/vertical samples in each tile\n",
    "\n",
    "#create backbone and rpn networks\n",
    "resnet50 = tf.keras.applications.ResNet50(\n",
    "    include_top=False, weights='imagenet', input_tensor=None,\n",
    "    input_shape=(train_tile, train_tile, 3), pooling=None)\n",
    "rpnetwork, backbone = rpn(resnet50, n_anchors=tf.size(anchor_px),\n",
    "                          stride=backbone_stride, blocks=backbone_blocks, \n",
    "                          kernels=rpn_kernel, dimensions=[backbone_dimension],\n",
    "                          activations=rpn_act_conv)\n",
    "\n",
    "#create FasterRCNN keras model\n",
    "faster_model = FasterRCNN(rpnetwork, backbone, [width, height], anchor_px, rpn_lmbda)\n",
    "\n",
    "#compile FasterRCNN model with losses\n",
    "faster_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss=[tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                    tf.keras.losses.Huber()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Weights of Faster R-CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faster_path = \"/tf/notebooks/networks/cpk_ly_3\"\n",
    "\n",
    "faster_model.load_weights(faster_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Faster R-CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=ds_train_roi, batch_size=1, epochs=1, verbose=1,\n",
    "          validation_data=ds_validation_roi, validation_freq=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LearningNMS configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_path = './histomics_detect/example/lnms_configs.yml'\n",
    "\n",
    "with open(configs_path) as config_file:\n",
    "  configs = yaml.safe_load(config_file)\n",
    "\n",
    "for key, value in configs['special_configs'].items():\n",
    "    try:\n",
    "      configs[key] = eval(value.replace('\\\\\\'', '\\''))\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "configs['roialign_pool'] = tf.cast(configs['roialign_pool'], tf.int32)\n",
    "configs['roialign_tiles'] = tf.cast(configs['roialign_tiles'], tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LearningNMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_net = CompressionNetwork(configs['feature_size'], configs['anchor_size'], faster_model.backbone)\n",
    "    \n",
    "model = LearningNMS(configs, faster_model.rpnetwork, faster_model.backbone, compression_net.compression_layers, \n",
    "                    [configs['width'], configs['height']], )\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-5,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule))\n",
    "\n",
    "\n",
    "# train model\n",
    "history_callback = model.fit(x=ds_train_roi, batch_size=1, epochs=1, verbose=1, steps_per_epoch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "save_fig = False\n",
    "fig_path = 'path/to/image.png'\n",
    "filter_edge = True\n",
    "\n",
    "run_plot(ds_validation_roi, model, index, save_fig, fig_path, filter_edge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
